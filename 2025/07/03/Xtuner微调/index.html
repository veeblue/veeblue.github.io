<!DOCTYPE html>
<html lang="en-US">
  <head prefix="og: https://ogp.me/ns#">
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="color-scheme" content="light dark">
  
  <title>Xtuner微调 - Veeblue</title>
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  <link rel='manifest' href='/manifest.json'>
  

  <meta name="description" content="1github: https:&#x2F;&#x2F;github.com&#x2F;InternLM&#x2F;xtuner  安装123456conda create --name xtuner-env python&#x3D;3.10 -yconda activate xtuner-envgit clone https:&#x2F;&#x2F;github.com&#x2F;InternLM&#x2F;xtuner.gitcd xtunerpip install -e &amp;#x27">
<meta property="og:type" content="article">
<meta property="og:title" content="Xtuner微调">
<meta property="og:url" content="https://blog.veeblue.com/2025/07/03/Xtuner%E5%BE%AE%E8%B0%83/index.html">
<meta property="og:site_name" content="Veeblue">
<meta property="og:description" content="1github: https:&#x2F;&#x2F;github.com&#x2F;InternLM&#x2F;xtuner  安装123456conda create --name xtuner-env python&#x3D;3.10 -yconda activate xtuner-envgit clone https:&#x2F;&#x2F;github.com&#x2F;InternLM&#x2F;xtuner.gitcd xtunerpip install -e &amp;#x27">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://blog.veeblue.com/images/%E6%88%AA%E5%B1%8F2025-06-27_14.27.35.png">
<meta property="og:image" content="https://blog.veeblue.com/images/%E6%88%AA%E5%B1%8F2025-06-27_14.29.09.png">
<meta property="og:image" content="https://blog.veeblue.com/images/%E6%88%AA%E5%B1%8F2025-06-28_13.43.38.png">
<meta property="og:image" content="https://blog.veeblue.com/images/%E6%88%AA%E5%B1%8F2025-06-28_13.47.24.png">
<meta property="article:published_time" content="2025-07-03T06:57:15.000Z">
<meta property="article:modified_time" content="2025-08-09T01:52:13.507Z">
<meta property="article:author" content="Veeblue">
<meta property="article:tag" content="Xtuner">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog.veeblue.com/images/%E6%88%AA%E5%B1%8F2025-06-27_14.27.35.png">

  <link rel="preconnect" href="https://rsms.me/">
  <link rel="stylesheet" href="https://rsms.me/inter/inter.css">
  
  
<link rel="stylesheet" href="https://unpkg.com/simple-icons-font@v13/font/simple-icons.min.css">

  
  
  
<link rel="stylesheet" href="https://unpkg.com/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">

  

  
<link rel="stylesheet" href="/css/var.css">

  
<link rel="stylesheet" href="/css/main.css">

  
<link rel="stylesheet" href="/css/typography.css">

  
<link rel="stylesheet" href="/css/code-highlighting.css">

  
  
  
<link rel="stylesheet" href="/css/components/card.css">

  
  
  
  
<link rel="stylesheet" href="/css/components/button.css">

  
  
  
  
<link rel="stylesheet" href="/css/components/badge.css">

  
  
  
  
<link rel="stylesheet" href="/css/components/utilities.css">

  
  
  
  
<link rel="stylesheet" href="/css/components/carousel.css">

  
  
  
<link rel="stylesheet" href="/css/nav.css">

  
<link rel="stylesheet" href="/css/paginator.css">

  
<link rel="stylesheet" href="/css/footer.css">

  
<link rel="stylesheet" href="/css/post-list.css">

  
  
<link rel="stylesheet" href="/css/rainbow-banner.css">

  
  
  
<link rel="stylesheet" href="/css/toc.css">

  
  
  
  
  
<link rel="stylesheet" href="/css/post.css">

  
  
  
  
<link rel="stylesheet" href="/pagefind/pagefind-ui.css">

  
<script src="/pagefind/pagefind-ui.js"></script>

  
<link rel="stylesheet" href="/css/search.css">

  
  
  
  
  
<link rel="stylesheet" href="/css/scroll-reveal.css">

  
  
  
<link rel="stylesheet" href="/css/view-transition.css">

  
  
  <script src="https://unpkg.com/lenis@1.1.9/dist/lenis.min.js"></script>
  
<link rel="stylesheet" href="https://unpkg.com/lenis@1.1.9/dist/lenis.css">

  
<script src="/js/smooth-scroll.js"></script>

  

  

  
<script src="/js/head.js"></script>

  
<meta name="generator" content="Hexo 7.3.0"></head>

  <body
    data-color-scheme="auto"
    data-uppercase-categories="true"
    
    data-rainbow-banner="true"
    data-rainbow-banner-shown="auto"
    data-rainbow-banner-month="6"
    data-rainbow-banner-colors="#e50000,#ff8d00,#ffee00,#008121,#004cff,#760188"
    
    data-config-root="/"
    
    data-toc="true"
    data-toc-max-depth="2"
    
    
    
    data-scroll-reveal-disappear="false"
    data-scroll-reveal-query=".scroll-reveal, .post-list-item, .card, .content p img, .content .block-large img"
    
    data-nav-blur-gradient="true"
  >
    
<script src="/js/body-top.js"></script>

    <a href="#main-content" id="skip-to-content">Skip to content</a>
    

<div class="nav-mask">
  <div order="1"></div>
  <div order="2"></div>
  <div order="3"></div>
  <div order="4"></div>
  <div order="5"></div>
  <div order="6"></div>
</div>

<nav id="theme-nav">
  <div class="inner">
    <a class="title" href="/">Blog</a>
    <div class="nav-arrow"></div>
    <div class="nav-items">
      <a class="nav-item nav-item-home" href="/" style="--index: 0">Home</a>

      
      
      <a class="nav-item" href="/archives" style="--index: 1">Archives</a>
      
      
      
      <a class="nav-item" href="/tags" style="--index: 2">Tags</a>
      
      
      
      <a class="nav-item" href="/categories" style="--index: 3">Categories</a>
      
      
      
      <a class="nav-item" href="/" style="--index: 4">About</a>
      
      
      
      <a class="nav-item is-icon" target="_blank" rel="noopener" href="https://github.com/veeblue" style="--index: 5"><i class="si si-github"></i></a>
      
      
      
      <a class="nav-item is-icon" target="_blank" rel="noopener" href="https://x.com/_veeblue" style="--index: 6"><i class="si si-x"></i></a>
      
      
      
      <button class="nav-item is-icon" onclick="openSearchModal()" style="--index: 7"><i class="bi bi-search"></i></button>
      
      
    </div>
  </div>
</nav>

    <main id="main-content" data-pagefind-body>
      
<article class="post">
  
  <div class="cover">
    <img src="/images/Blog-Banner.png" alt="" class="cover-img" data-pagefind-meta="image[src], image_alt[alt]" />
  </div>
  
  <div class="meta">
    
    <div class="categories text-uppercase">
    
      <a href="/categories/NOTE/">NOTE</a>
    
    </div>
    

    
    <time class="date" datetime="2025-07-03T14:57:15+08:00">
      July 3, 2025
    </time>
    

    <h1 class="title" data-pagefind-meta="title">Xtuner微调</h1>
  </div>

  <div class="content">
    <figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">github</span>: <span class="attr">https</span>:<span class="comment">//github.com/InternLM/xtuner</span></span><br></pre></td></tr></table></figure>

<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">conda create --name xtuner-env python=<span class="number">3.10</span> -y</span><br><span class="line">conda activate xtuner-env</span><br><span class="line"></span><br><span class="line">git clone <span class="attr">https</span>:<span class="comment">//github.com/InternLM/xtuner.git</span></span><br><span class="line">cd xtuner</span><br><span class="line">pip install -e <span class="string">&#x27;.[all]&#x27;</span></span><br></pre></td></tr></table></figure>

<h1 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h1><p>目录：xtuner&#x2F;xuner&#x2F;configs&#x2F;[对应的模型]&#x2F;[对应的模型配置].py</p>
<p>配置内容（QLora为例）：</p>
<p>下载对应的模型：</p>
<figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> modelscope <span class="keyword">import</span> snapshot_download</span><br><span class="line">model_dir = <span class="title function_">snapshot_download</span>(<span class="string">&#x27;Qwen/Qwen1.5-1.8B-Chat&#x27;</span>, cache_dir=<span class="string">&#x27;autodl-tmp/models&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>将模型配置文件复制到根目录（代码目录）：</p>
<p>修改参数：</p>
<figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br></pre></td><td class="code"><pre><span class="line"># <span class="title class_">Copyright</span> (c) <span class="title class_">OpenMMLab</span>. <span class="title class_">All</span> rights reserved.</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">from</span> mmengine.<span class="property">dataset</span> <span class="keyword">import</span> <span class="title class_">DefaultSampler</span></span><br><span class="line"><span class="keyword">from</span> mmengine.<span class="property">hooks</span> <span class="keyword">import</span> (</span><br><span class="line">    <span class="title class_">CheckpointHook</span>,</span><br><span class="line">    <span class="title class_">DistSamplerSeedHook</span>,</span><br><span class="line">    <span class="title class_">IterTimerHook</span>,</span><br><span class="line">    <span class="title class_">LoggerHook</span>,</span><br><span class="line">    <span class="title class_">ParamSchedulerHook</span>,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> mmengine.<span class="property">optim</span> <span class="keyword">import</span> <span class="title class_">AmpOptimWrapper</span>, <span class="title class_">CosineAnnealingLR</span>, <span class="title class_">LinearLR</span></span><br><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> <span class="title class_">LoraConfig</span></span><br><span class="line"><span class="keyword">from</span> torch.<span class="property">optim</span> <span class="keyword">import</span> <span class="title class_">AdamW</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> <span class="title class_">AutoModelForCausalLM</span>, <span class="title class_">AutoTokenizer</span>, <span class="title class_">BitsAndBytesConfig</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> xtuner.<span class="property">dataset</span> <span class="keyword">import</span> process_hf_dataset</span><br><span class="line"><span class="keyword">from</span> xtuner.<span class="property">dataset</span>.<span class="property">collate_fns</span> <span class="keyword">import</span> default_collate_fn</span><br><span class="line"><span class="keyword">from</span> xtuner.<span class="property">dataset</span>.<span class="property">map_fns</span> <span class="keyword">import</span> alpaca_map_fn, template_map_fn_factory</span><br><span class="line"><span class="keyword">from</span> xtuner.<span class="property">engine</span>.<span class="property">hooks</span> <span class="keyword">import</span> (</span><br><span class="line">    <span class="title class_">DatasetInfoHook</span>,</span><br><span class="line">    <span class="title class_">EvaluateChatHook</span>,</span><br><span class="line">    <span class="title class_">VarlenAttnArgsToMessageHubHook</span>,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> xtuner.<span class="property">engine</span>.<span class="property">runner</span> <span class="keyword">import</span> <span class="title class_">TrainLoop</span></span><br><span class="line"><span class="keyword">from</span> xtuner.<span class="property">model</span> <span class="keyword">import</span> <span class="title class_">SupervisedFinetune</span></span><br><span class="line"><span class="keyword">from</span> xtuner.<span class="property">parallel</span>.<span class="property">sequence</span> <span class="keyword">import</span> <span class="title class_">SequenceParallelSampler</span></span><br><span class="line"><span class="keyword">from</span> xtuner.<span class="property">utils</span> <span class="keyword">import</span> <span class="variable constant_">PROMPT_TEMPLATE</span>, <span class="variable constant_">SYSTEM_TEMPLATE</span></span><br><span class="line"></span><br><span class="line">#######################################################################</span><br><span class="line">#                          <span class="variable constant_">PART</span> <span class="number">1</span>  <span class="title class_">Settings</span>                           #</span><br><span class="line">#######################################################################</span><br><span class="line">**# <span class="title class_">Model</span> 修改模型路径**</span><br><span class="line">**pretrained_model_name_or_path = <span class="string">&quot;/root/autodl-tmp/models/Qwen/Qwen1___5-1___8B-Chat&quot;</span>**</span><br><span class="line">use_varlen_attn = <span class="title class_">False</span></span><br><span class="line"></span><br><span class="line"># <span class="title class_">Data</span></span><br><span class="line">**alpaca_en_path = <span class="string">&quot;/root/autodl-tmp/code/data/train_data.json&quot;</span> # 自定的json**</span><br><span class="line">prompt_template = <span class="variable constant_">PROMPT_TEMPLATE</span>.<span class="property">qwen_chat</span></span><br><span class="line">**max_length = <span class="number">512</span>**</span><br><span class="line">pack_to_max_length = <span class="title class_">True</span></span><br><span class="line"></span><br><span class="line"># parallel</span><br><span class="line">sequence_parallel_size = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"># <span class="title class_">Scheduler</span> &amp; <span class="title class_">Optimizer</span></span><br><span class="line">**batch_size = <span class="number">6</span>**  # per_device</span><br><span class="line">accumulative_counts = <span class="number">16</span></span><br><span class="line">accumulative_counts *= sequence_parallel_size</span><br><span class="line">dataloader_num_workers = <span class="number">0</span></span><br><span class="line">**max_epochs = <span class="number">10</span>**</span><br><span class="line">optim_type = <span class="title class_">AdamW</span></span><br><span class="line">lr = <span class="number">2e-4</span></span><br><span class="line">betas = (<span class="number">0.9</span>, <span class="number">0.999</span>)</span><br><span class="line">weight_decay = <span class="number">0</span></span><br><span class="line">max_norm = <span class="number">1</span>  # grad clip</span><br><span class="line">warmup_ratio = <span class="number">0.03</span></span><br><span class="line"></span><br><span class="line"># <span class="title class_">Save</span></span><br><span class="line">save_steps = <span class="number">500</span></span><br><span class="line">save_total_limit = <span class="number">2</span>  # <span class="title class_">Maximum</span> checkpoints to <span class="title function_">keep</span> (-<span class="number">1</span> means unlimited)</span><br><span class="line"></span><br><span class="line"># <span class="title class_">Evaluate</span> the generation performance during the training</span><br><span class="line">evaluation_freq = <span class="number">500</span></span><br><span class="line">evaluation_freq = <span class="number">500</span></span><br><span class="line">**<span class="variable constant_">SYSTEM</span> = <span class="string">&#x27;你由Veeblue打造的中文领域心理健康助手, 是一个研究过无数具有心理健康问题的病人与心理健康医生对话的心理专家, 在心理方面拥有广博的知识储备和丰富的研究咨询经验，你旨在通过专业心理咨询, 协助来访者完成心理诊断。请充分利用专业心理学知识与咨询技术, 一步步帮助来访者解决心理问题, 接下来你将只使用中文来回答和咨询问题。&#x27;</span></span><br><span class="line">evaluation_inputs = [</span><br><span class="line"><span class="string">&#x27;请介绍你自己&#x27;</span>, # self cognition</span><br><span class="line"><span class="string">&#x27;你好&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;我今天心情不好，感觉不开心，很烦。&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;我最近总是感到很焦虑，尤其是在学业上。我有个特别崇拜的同学，他好像在各方面都比我优秀，我总觉得自己怎么努力也追不上他，这让我压力特别大。&#x27;</span>,</span><br><span class="line">]**</span><br><span class="line"></span><br><span class="line">#######################################################################</span><br><span class="line">#                      <span class="variable constant_">PART</span> <span class="number">2</span>  <span class="title class_">Model</span> &amp; <span class="title class_">Tokenizer</span>                      #</span><br><span class="line">#######################################################################</span><br><span class="line">tokenizer = <span class="title function_">dict</span>(</span><br><span class="line">    type=<span class="title class_">AutoTokenizer</span>.<span class="property">from_pretrained</span>,</span><br><span class="line">    pretrained_model_name_or_path=pretrained_model_name_or_path,</span><br><span class="line">    trust_remote_code=<span class="title class_">True</span>,</span><br><span class="line">    padding_side=<span class="string">&quot;right&quot;</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">model = <span class="title function_">dict</span>(</span><br><span class="line">    type=<span class="title class_">SupervisedFinetune</span>,</span><br><span class="line">    use_varlen_attn=use_varlen_attn,</span><br><span class="line">    llm=<span class="title function_">dict</span>(</span><br><span class="line">        type=<span class="title class_">AutoModelForCausalLM</span>.<span class="property">from_pretrained</span>,</span><br><span class="line">        pretrained_model_name_or_path=pretrained_model_name_or_path,</span><br><span class="line">        trust_remote_code=<span class="title class_">True</span>,</span><br><span class="line">        torch_dtype=torch.<span class="property">float16</span>,</span><br><span class="line">        **quantization_config=<span class="title class_">None</span>,  # 设置<span class="title class_">None</span>不启用<span class="title class_">QLora</span>微调</span><br><span class="line">        # 若启用<span class="title class_">QLora</span>：</span><br><span class="line">        # <span class="title function_">dict</span>(</span><br><span class="line">        #     type=<span class="title class_">BitsAndBytesConfig</span>,</span><br><span class="line">        #     load_in_4bit=<span class="title class_">True</span>,</span><br><span class="line">        #     load_in_8bit=<span class="title class_">False</span>,</span><br><span class="line">        #     llm_int8_threshold=<span class="number">6.0</span>,</span><br><span class="line">        #     llm_int8_has_fp16_weight=<span class="title class_">False</span>,</span><br><span class="line">        #     bnb_4bit_compute_dtype=torch.<span class="property">float16</span>,</span><br><span class="line">        #     bnb_4bit_use_double_quant=<span class="title class_">True</span>,</span><br><span class="line">        #     bnb_4bit_quant_type=<span class="string">&quot;nf4&quot;</span>,</span><br><span class="line">        # ),**</span><br><span class="line">    ),</span><br><span class="line">    lora=<span class="title function_">dict</span>(</span><br><span class="line">        type=<span class="title class_">LoraConfig</span>,</span><br><span class="line">        r=<span class="number">64</span>,</span><br><span class="line">        lora_alpha=<span class="number">16</span>,</span><br><span class="line">        lora_dropout=<span class="number">0.1</span>,</span><br><span class="line">        bias=<span class="string">&quot;none&quot;</span>,</span><br><span class="line">        task_type=<span class="string">&quot;CAUSAL_LM&quot;</span>,</span><br><span class="line">    ),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">#######################################################################</span><br><span class="line">#                      <span class="variable constant_">PART</span> <span class="number">3</span>  <span class="title class_">Dataset</span> &amp; <span class="title class_">Dataloader</span>                   #</span><br><span class="line">#######################################################################</span><br><span class="line">alpaca_en = <span class="title function_">dict</span>(</span><br><span class="line">    type=process_hf_dataset,</span><br><span class="line">    **dataset=<span class="title function_">dict</span>(type=load_dataset, path=<span class="string">&#x27;json&#x27;</span>, data_files=alpaca_en_path), #使用自定义的json**</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">    max_length=max_length,</span><br><span class="line">    **dataset_map_fn=<span class="title class_">None</span>,**</span><br><span class="line">    template_map_fn=<span class="title function_">dict</span>(type=template_map_fn_factory, template=prompt_template),</span><br><span class="line">    remove_unused_columns=<span class="title class_">True</span>,</span><br><span class="line">    shuffle_before_pack=<span class="title class_">True</span>,</span><br><span class="line">    pack_to_max_length=pack_to_max_length,</span><br><span class="line">    use_varlen_attn=use_varlen_attn,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">sampler = <span class="title class_">SequenceParallelSampler</span> <span class="keyword">if</span> sequence_parallel_size &gt; <span class="number">1</span> <span class="keyword">else</span> <span class="title class_">DefaultSampler</span></span><br><span class="line"></span><br><span class="line">train_dataloader = <span class="title function_">dict</span>(</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    num_workers=dataloader_num_workers,</span><br><span class="line">    dataset=alpaca_en,</span><br><span class="line">    sampler=<span class="title function_">dict</span>(type=sampler, shuffle=<span class="title class_">True</span>),</span><br><span class="line">    collate_fn=<span class="title function_">dict</span>(type=default_collate_fn, use_varlen_attn=use_varlen_attn),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">#######################################################################</span><br><span class="line">#                    <span class="variable constant_">PART</span> <span class="number">4</span>  <span class="title class_">Scheduler</span> &amp; <span class="title class_">Optimizer</span>                    #</span><br><span class="line">#######################################################################</span><br><span class="line"># optimizer</span><br><span class="line">optim_wrapper = <span class="title function_">dict</span>(</span><br><span class="line">    type=<span class="title class_">AmpOptimWrapper</span>,</span><br><span class="line">    optimizer=<span class="title function_">dict</span>(type=optim_type, lr=lr, betas=betas, weight_decay=weight_decay),</span><br><span class="line">    clip_grad=<span class="title function_">dict</span>(max_norm=max_norm, error_if_nonfinite=<span class="title class_">False</span>),</span><br><span class="line">    accumulative_counts=accumulative_counts,</span><br><span class="line">    loss_scale=<span class="string">&quot;dynamic&quot;</span>,</span><br><span class="line">    dtype=<span class="string">&quot;float16&quot;</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># learning policy</span><br><span class="line"># <span class="title class_">More</span> <span class="attr">information</span>: <span class="attr">https</span>:<span class="comment">//github.com/open-mmlab/mmengine/blob/main/docs/en/tutorials/param_scheduler.md  # noqa: E501</span></span><br><span class="line">param_scheduler = [</span><br><span class="line">    <span class="title function_">dict</span>(</span><br><span class="line">        type=<span class="title class_">LinearLR</span>,</span><br><span class="line">        start_factor=<span class="number">1e-5</span>,</span><br><span class="line">        by_epoch=<span class="title class_">True</span>,</span><br><span class="line">        begin=<span class="number">0</span>,</span><br><span class="line">        end=warmup_ratio * max_epochs,</span><br><span class="line">        convert_to_iter_based=<span class="title class_">True</span>,</span><br><span class="line">    ),</span><br><span class="line">    <span class="title function_">dict</span>(</span><br><span class="line">        type=<span class="title class_">CosineAnnealingLR</span>,</span><br><span class="line">        eta_min=<span class="number">0.0</span>,</span><br><span class="line">        by_epoch=<span class="title class_">True</span>,</span><br><span class="line">        begin=warmup_ratio * max_epochs,</span><br><span class="line">        end=max_epochs,</span><br><span class="line">        convert_to_iter_based=<span class="title class_">True</span>,</span><br><span class="line">    ),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"># train, val, test setting</span><br><span class="line">train_cfg = <span class="title function_">dict</span>(type=<span class="title class_">TrainLoop</span>, max_epochs=max_epochs)</span><br><span class="line"></span><br><span class="line">#######################################################################</span><br><span class="line">#                           <span class="variable constant_">PART</span> <span class="number">5</span>  <span class="title class_">Runtime</span>                           #</span><br><span class="line">#######################################################################</span><br><span class="line"># <span class="title class_">Log</span> the dialogue periodically during the training process, optional</span><br><span class="line">custom_hooks = [</span><br><span class="line">    <span class="title function_">dict</span>(type=<span class="title class_">DatasetInfoHook</span>, tokenizer=tokenizer),</span><br><span class="line">    <span class="title function_">dict</span>(</span><br><span class="line">        type=<span class="title class_">EvaluateChatHook</span>,</span><br><span class="line">        tokenizer=tokenizer,</span><br><span class="line">        every_n_iters=evaluation_freq,</span><br><span class="line">        evaluation_inputs=evaluation_inputs,</span><br><span class="line">        system=<span class="variable constant_">SYSTEM</span>,</span><br><span class="line">        prompt_template=prompt_template,</span><br><span class="line">    ),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="attr">use_varlen_attn</span>:</span><br><span class="line">    custom_hooks += [<span class="title function_">dict</span>(type=<span class="title class_">VarlenAttnArgsToMessageHubHook</span>)]</span><br><span class="line"></span><br><span class="line"># configure <span class="keyword">default</span> hooks</span><br><span class="line">default_hooks = <span class="title function_">dict</span>(</span><br><span class="line">    # record the time <span class="keyword">of</span> every iteration.</span><br><span class="line">    timer=<span class="title function_">dict</span>(type=<span class="title class_">IterTimerHook</span>),</span><br><span class="line">    # print log every <span class="number">10</span> iterations.</span><br><span class="line">    logger=<span class="title function_">dict</span>(type=<span class="title class_">LoggerHook</span>, log_metric_by_epoch=<span class="title class_">False</span>, interval=<span class="number">10</span>),</span><br><span class="line">    # enable the parameter scheduler.</span><br><span class="line">    param_scheduler=<span class="title function_">dict</span>(type=<span class="title class_">ParamSchedulerHook</span>),</span><br><span class="line">    # save checkpoint per <span class="string">`save_steps`</span>.</span><br><span class="line">    checkpoint=<span class="title function_">dict</span>(</span><br><span class="line">        type=<span class="title class_">CheckpointHook</span>,</span><br><span class="line">        by_epoch=<span class="title class_">False</span>,</span><br><span class="line">        interval=save_steps,</span><br><span class="line">        max_keep_ckpts=save_total_limit,</span><br><span class="line">    ),</span><br><span class="line">    # set sampler seed <span class="keyword">in</span> distributed evrionment.</span><br><span class="line">    sampler_seed=<span class="title function_">dict</span>(type=<span class="title class_">DistSamplerSeedHook</span>),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># configure environment</span><br><span class="line">env_cfg = <span class="title function_">dict</span>(</span><br><span class="line">    # whether to enable cudnn benchmark</span><br><span class="line">    cudnn_benchmark=<span class="title class_">False</span>,</span><br><span class="line">    # set multi process parameters</span><br><span class="line">    mp_cfg=<span class="title function_">dict</span>(mp_start_method=<span class="string">&quot;fork&quot;</span>, opencv_num_threads=<span class="number">0</span>),</span><br><span class="line">    # set distributed parameters</span><br><span class="line">    dist_cfg=<span class="title function_">dict</span>(backend=<span class="string">&quot;nccl&quot;</span>),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># set visualizer</span><br><span class="line">visualizer = <span class="title class_">None</span></span><br><span class="line"></span><br><span class="line"># set log level</span><br><span class="line">log_level = <span class="string">&quot;INFO&quot;</span></span><br><span class="line"></span><br><span class="line"># load <span class="keyword">from</span> which checkpoint</span><br><span class="line">load_from = <span class="title class_">None</span></span><br><span class="line"></span><br><span class="line"># whether to resume training <span class="keyword">from</span> the loaded checkpoint</span><br><span class="line">resume = <span class="title class_">False</span></span><br><span class="line"></span><br><span class="line"># <span class="title class_">Defaults</span> to use random seed and disable <span class="string">`deterministic`</span></span><br><span class="line">randomness = <span class="title function_">dict</span>(seed=<span class="title class_">None</span>, deterministic=<span class="title class_">False</span>)</span><br><span class="line"></span><br><span class="line"># set log processor</span><br><span class="line">log_processor = <span class="title function_">dict</span>(by_epoch=<span class="title class_">False</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>其中<strong>train_data.json：</strong></p>
<figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;conversation&quot;</span>: [</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">&quot;input&quot;</span>: <span class="string">&quot;最近总是觉得自己很焦虑，感觉压力很大，怎么办呢？&quot;</span>,</span><br><span class="line">                <span class="string">&quot;output&quot;</span>: <span class="string">&quot;我很明白你的感受，焦虑和压力都是很正常的情绪，但是我们可以一起想办法来缓解它们。&quot;</span></span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">&quot;input&quot;</span>: <span class="string">&quot;可是我总是感觉自己做的不够好，压力和焦虑情绪就会更加加重，怎么破？&quot;</span>,</span><br><span class="line">                <span class="string">&quot;output&quot;</span>: <span class="string">&quot;别太苛责自己了，我们都是人，总有做得不够好的时候。也许你可以试着规划一下自己的计划，分解成小目标，逐步完成，让自己的进步更可见。&quot;</span></span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">&quot;input&quot;</span>: <span class="string">&quot;可是我总是会拖延，根本不能按照计划执行，怎么办呢？&quot;</span>,</span><br><span class="line">                <span class="string">&quot;output&quot;</span>: <span class="string">&quot;是啊，拖延也是很常见的问题。你可以尝试用时间管理的方法，设定时间表和截止时间，提醒自己要按照计划去行动。&quot;</span></span><br><span class="line">            &#125;,</span><br><span class="line">            ......</span><br><span class="line">            </span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p>运行：</p>
<figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xtuner train qwen1_5_1_8b_chat_qlora_alpaca_e3.<span class="property">py</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p><strong>报错：ModuleNotFoundError: No module named ‘triton.ops’</strong></p>
</blockquote>
<p><strong>解决：</strong><code>pip install --upgrade bitsandbytes</code></p>
<p><strong>后台训练</strong>：使用<code>screen</code>命令开启，<code>ctl + a + d</code>会退出到原终端，并且显示detached，意味着这个会话只是离开并未退出。</p>
<p><strong>重进入会话：</strong><code>screen -ls</code></p>
<p><img src="/images/%E6%88%AA%E5%B1%8F2025-06-27_14.27.35.png" alt="截屏2025-06-27 14.27.35.png"></p>
<p><strong>恢复会话</strong>：screen -r 【会话ID】</p>
<p><img src="/images/%E6%88%AA%E5%B1%8F2025-06-27_14.29.09.png" alt="截屏2025-06-27 14.29.09.png"></p>
<p>**模型转换：**pth —&gt; hf</p>
<p><strong><code>xtuner convert pth_to_hf $CONFIG_NAME_OR_PATH $PTH $SAVE_PATH</code></strong></p>
<p><code>xtuner convert pth_to_hf ./qwen1_5_1_8b_chat_qlora_alpaca_e3.py ./iter_9560.pth ./hf</code></p>
<p><strong>转换出错：</strong></p>
<figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">(xtuner-env) root@autodl-container-e1ee44a95d-<span class="attr">c3292730</span>:~<span class="regexp">/autodl-tmp/</span>code/work_dirs/qwen1_5_1_8b_chat_qlora_alpaca_e3# xtuner convert pth_to_hf ./qwen1_5_1_8b_chat_qlora_alpaca_e3.<span class="property">py</span> ./iter_9560.<span class="property">pth</span> ./hf</span><br><span class="line">[<span class="number">2025</span>-<span class="number">06</span>-<span class="number">28</span> <span class="number">13</span>:<span class="number">28</span>:<span class="number">45</span>,<span class="number">006</span>] [<span class="variable constant_">INFO</span>] [real_accelerator.<span class="property">py</span>:<span class="number">222</span>:get_accelerator] <span class="title class_">Setting</span> ds_accelerator to <span class="title function_">cuda</span> (auto detect)</span><br><span class="line">[<span class="number">2025</span>-<span class="number">06</span>-<span class="number">28</span> <span class="number">13</span>:<span class="number">28</span>:<span class="number">49</span>,<span class="number">879</span>] [<span class="variable constant_">INFO</span>] [real_accelerator.<span class="property">py</span>:<span class="number">222</span>:get_accelerator] <span class="title class_">Setting</span> ds_accelerator to <span class="title function_">cuda</span> (auto detect)</span><br><span class="line"><span class="title class_">Traceback</span> (most recent call last):</span><br><span class="line">  <span class="title class_">File</span> <span class="string">&quot;/root/autodl-tmp/xtuner/xtuner/tools/model_converters/pth_to_hf.py&quot;</span>, line <span class="number">151</span>, <span class="keyword">in</span> &lt;<span class="variable language_">module</span>&gt;</span><br><span class="line">    <span class="title function_">main</span>()</span><br><span class="line">  <span class="title class_">File</span> <span class="string">&quot;/root/autodl-tmp/xtuner/xtuner/tools/model_converters/pth_to_hf.py&quot;</span>, line <span class="number">123</span>, <span class="keyword">in</span> main</span><br><span class="line">    state_dict = <span class="title function_">guess_load_checkpoint</span>(args.<span class="property">pth_model</span>)</span><br><span class="line">  <span class="title class_">File</span> <span class="string">&quot;/root/autodl-tmp/xtuner/xtuner/model/utils.py&quot;</span>, line <span class="number">314</span>, <span class="keyword">in</span> guess_load_checkpoint</span><br><span class="line">    state_dict = torch.<span class="title function_">load</span>(pth_model, map_location=<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">  <span class="title class_">File</span> <span class="string">&quot;/root/miniconda3/envs/xtuner-env/lib/python3.10/site-packages/torch/serialization.py&quot;</span>, line <span class="number">1524</span>, <span class="keyword">in</span> load</span><br><span class="line">    raise pickle.<span class="title class_">UnpicklingError</span>(<span class="title function_">_get_wo_message</span>(<span class="title function_">str</span>(e))) <span class="keyword">from</span> <span class="title class_">None</span></span><br><span class="line">_pickle.<span class="property">UnpicklingError</span>: <span class="title class_">Weights</span> only load failed. <span class="title class_">This</span> file can still be loaded, to <span class="keyword">do</span> so you have two options, <span class="keyword">do</span> those steps only <span class="keyword">if</span> you trust the source <span class="keyword">of</span> the checkpoint. </span><br><span class="line">        (<span class="number">1</span>) <span class="title class_">In</span> <span class="title class_">PyTorch</span> <span class="number">2.6</span>, we changed the <span class="keyword">default</span> value <span class="keyword">of</span> the weights_only argument <span class="keyword">in</span> torch.<span class="property">load</span> <span class="keyword">from</span> <span class="title class_">False</span> to <span class="title class_">True</span>. <span class="title class_">Re</span>-running torch.<span class="property">load</span> <span class="keyword">with</span> weights_only set to <span class="title class_">False</span> will likely succeed, but it can result <span class="keyword">in</span> arbitrary code execution. <span class="title class_">Do</span> it only <span class="keyword">if</span> you got the file <span class="keyword">from</span> a trusted source.</span><br><span class="line">        (<span class="number">2</span>) <span class="title class_">Alternatively</span>, to load <span class="keyword">with</span> weights_only=<span class="title class_">True</span> please check the recommended steps <span class="keyword">in</span> the following error message.</span><br><span class="line">        <span class="title class_">WeightsUnpickler</span> <span class="attr">error</span>: <span class="title class_">Unsupported</span> <span class="attr">global</span>: <span class="variable constant_">GLOBAL</span> mmengine.<span class="property">logging</span>.<span class="property">history_buffer</span>.<span class="property">HistoryBuffer</span> was not an allowed <span class="variable language_">global</span> by <span class="keyword">default</span>. <span class="title class_">Please</span> use torch.<span class="property">serialization</span>.<span class="title function_">add_safe_globals</span>([mmengine.<span class="property">logging</span>.<span class="property">history_buffer</span>.<span class="property">HistoryBuffer</span>]) or the torch.<span class="property">serialization</span>.<span class="title function_">safe_globals</span>([mmengine.<span class="property">logging</span>.<span class="property">history_buffer</span>.<span class="property">HistoryBuffer</span>]) context manager to allowlist <span class="variable language_">this</span> <span class="variable language_">global</span> <span class="keyword">if</span> you trust <span class="variable language_">this</span> <span class="keyword">class</span>/<span class="keyword">function</span>.</span><br><span class="line"><span class="title class_">Check</span> the documentation <span class="keyword">of</span> torch.<span class="property">load</span> to learn more about types accepted by <span class="keyword">default</span> <span class="keyword">with</span> weights_only <span class="attr">https</span>:<span class="comment">//pytorch.org/docs/stable/generated/torch.load.html.</span></span><br></pre></td></tr></table></figure>

<p><strong>原因：</strong></p>
<p>这个错误是由于 PyTorch 2.6 改变了 <code>torch.load</code> 的默认行为导致的。新版本默认启用了 <code>weights_only=True</code> 参数以提高安全性，但这导致某些包含非标准对象的检查点文件无法加载。</p>
<p><strong>解决：</strong></p>
<p>修改 xtuner 源码（推荐）找到错误提示中的文件 <code>/root/autodl-tmp/xtuner/xtuner/model/utils.py</code>，在第314行附近修改 <code>torch.load</code> 调用：</p>
<figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 原来的代码</span><br><span class="line">state_dict = torch.<span class="title function_">load</span>(pth_model, map_location=<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"># 修改为</span><br><span class="line">state_dict = torch.<span class="title function_">load</span>(pth_model, map_location=<span class="string">&quot;cpu&quot;</span>, weights_only=<span class="title class_">False</span>)</span><br></pre></td></tr></table></figure>

<p><strong>再次执行：</strong></p>
<p><img src="/images/%E6%88%AA%E5%B1%8F2025-06-28_13.43.38.png" alt="截屏2025-06-28 13.43.38.png"></p>
<p><strong>模型合并：</strong></p>
<p><strong><code>xtuner convert merge /root/autodl-tmp/models/Qwen/Qwen1___5-1___8B-Chat ./hf ./merged --max-shard-size 2GB</code></strong></p>
<p><img src="/images/%E6%88%AA%E5%B1%8F2025-06-28_13.47.24.png" alt="截屏2025-06-28 13.47.24.png"></p>
<p><strong>模型聊天：</strong></p>
<p><code>xtuner chat ./merged --prompt-template qwen_chat</code></p>
<blockquote>
<p>**<code>--prompt-template 参数选择:** choose from &#39;default&#39;, &#39;zephyr&#39;, &#39;internlm_chat&#39;, &#39;internlm2_chat&#39;, &#39;moss_sft&#39;, &#39;llama2_chat&#39;, &#39;code_llama_chat&#39;, &#39;chatglm2&#39;, &#39;chatglm3&#39;, &#39;qwen_chat&#39;, &#39;baichuan_chat&#39;, &#39;baichuan2_chat&#39;, &#39;wizardlm&#39;, &#39;wizardcoder&#39;, &#39;vicuna&#39;, &#39;deepseek_coder&#39;, &#39;deepseekcoder&#39;, &#39;deepseek_moe&#39;, &#39;deepseek_v2&#39;, &#39;mistral&#39;, &#39;mixtral&#39;, &#39;minicpm&#39;, &#39;minicpm3&#39;, &#39;gemma&#39;, &#39;cohere_chat&#39;, &#39;llama3_chat&#39;, &#39;phi3_chat’</code></p>
</blockquote>

  </div>

  
  <div class="about" data-pagefind-ignore>
    <h1>About this Post</h1>
    <div class="details">
      <p>This post is written by Veeblue, licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc/4.0">CC BY-NC 4.0</a>.</p>
    </div>
    
    <p class="tags">
      <i class="bi bi-tags icon"></i>
      
      <a href="/tags/Xtuner/" class="tag">#Xtuner</a>
      
    </p>
    
    
  </div>
  

  <div class="container post-prev-next" data-pagefind-ignore>
    
    <a href="/2025/07/03/%E9%83%A8%E7%BD%B2-vLLM-Open-WebUI/" class="next">
      <div class="text">
        <p class="label">Next</p>
        <h3 class="title">部署:vLLM + Open WebUI</h3>
      </div>
    </a>
    
    
    <a href="/2025/07/02/2025%E6%90%AD%E5%BB%BA-github-Hexo-%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/" class="prev">
      <div class="text">
        <p class="label">Previous</p>
        <h3 class="title">2025搭建 github + Hexo 个人博客</h3>
      </div>
    </a>
    
  </div>

  
    
    
  
</article>


    </main>
    <footer>
  <div class="inner">
    <div class="links">
      
      <div class="group">
        <h2 class="title">Blog</h2>
        
        <a href="/" class="item">Home</a>
        
        <a href="/archives" class="item">Archives</a>
        
        <a href="/tags" class="item">Tags</a>
        
        <a href="/categories" class="item">Categories</a>
        
        <a href="/search" class="item">Search</a>
        
        <a href="/friends" class="item">Friends</a>
        
        <a href="/projects" class="item">Projects</a>
        
        <a href="/resume" class="item">Resume</a>
        
        <a href="/" class="item">About</a>
        
        <a href="/atom.xml" class="item">RSS</a>
        
      </div>
      
      <div class="group">
        <h2 class="title">Me</h2>
        
        <a target="_blank" rel="noopener" href="https://github.com/veeblue" class="item">GitHub</a>
        
        <a href="mailto:yee@veeblue.com" class="item">Email</a>
        
      </div>
      
    </div>
    <span>&copy; 2025 Veeblue<br>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></span>
    
    
      <br>
      <div class="color-scheme-toggle" role="radiogroup" id="theme-color-scheme-toggle">
        <label>
          <input type="radio" value="light">
          <span>Light</span>
        </label>
        <label>
          <input type="radio" value="dark">
          <span>Dark</span>
        </label>
        <label>
          <input type="radio" value="auto">
          <span>Auto</span>
        </label>
      </div>
    
  </div>
</footer>


    <!-- Search Modal -->
    <div id="search-modal" class="search-modal">
      <div class="search-modal-content">
        <div class="search-modal-header">
          <h2>Search</h2>
          <button id="search-modal-close" class="search-modal-close">
            <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
              <line x1="18" y1="6" x2="6" y2="18"></line>
              <line x1="6" y1="6" x2="18" y2="18"></line>
            </svg>
          </button>
        </div>
        <div class="search-modal-body">
          <div id="search-modal-input"></div>
        </div>
      </div>
    </div>

    
<script src="/js/main.js"></script>

    
<script src="/js/code-blocks.js"></script>


    
    
<script src="/js/search.js"></script>

    

    
    
<script src="/js/scroll-reveal.js"></script>

    

    

    

    
  </body>
</html>
