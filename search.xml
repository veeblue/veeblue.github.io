<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>🧭 macOS Claude Code(使用Kimi Api)全流程安装指南</title>
      <link href="/2025/07/19/%F0%9F%A7%AD-macOS-Claude-Code-%E5%9F%BA%E4%BA%8EKIMI-%E5%85%A8%E6%B5%81%E7%A8%8B%E5%AE%89%E8%A3%85%E6%8C%87%E5%8D%97/"/>
      <url>/2025/07/19/%F0%9F%A7%AD-macOS-Claude-Code-%E5%9F%BA%E4%BA%8EKIMI-%E5%85%A8%E6%B5%81%E7%A8%8B%E5%AE%89%E8%A3%85%E6%8C%87%E5%8D%97/</url>
      
        <content type="html"><![CDATA[<blockquote><p>以下操作默认你已具备🪄<strong>科学上网</strong>的条件以及已经安装了🍺<code>Homebrew</code></p></blockquote><h3 id="🧩-安装Node-js"><a href="#🧩-安装Node-js" class="headerlink" title="🧩 安装Node.js"></a>🧩 安装Node.js</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install node</span><br></pre></td></tr></table></figure><h3 id="🧠-安装Claude-Code"><a href="#🧠-安装Claude-Code" class="headerlink" title="🧠 安装Claude Code"></a>🧠 安装Claude Code</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install -g @anthropic-ai/claude-code</span><br></pre></td></tr></table></figure><h3 id="🔑-申请Kimi-API-Key"><a href="#🔑-申请Kimi-API-Key" class="headerlink" title="🔑 申请Kimi API Key"></a>🔑 申请Kimi API Key</h3><p><a href="https://platform.moonshot.cn/console">🔗 点我直达申请</a></p><p><img src="/images/iShot_2025-07-18_18.45.22.png" alt="iShot_2025-07-18_18.45.22"></p><p><img src="/images/iShot_2025-07-18_18.45.58.png" alt="iShot_2025-07-18_18.45.58"></p><blockquote><p>🔔保存好你的<code>Api Key</code>,页面只会显示一次！</p></blockquote><h3 id="💻-配置kimi-api环境变量"><a href="#💻-配置kimi-api环境变量" class="headerlink" title="💻 配置kimi api环境变量"></a>💻 配置kimi api环境变量</h3><ul><li><p>打开终端输入：<code>vim ~/.zshrc</code></p></li><li><p>把以下内容追加到<code>.zshrc</code>文件里：</p></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export ANTHROPIC_BASE_URL=&quot;https://api.moonshot.cn/anthropic&quot;</span><br><span class="line">export ANTHROPIC_AUTH_TOKEN=&quot;你的Kimi API Key&quot;</span><br></pre></td></tr></table></figure><p><img src="/images/iShot_2025-07-18_18.38.23.png" alt="iShot_2025-07-18_18.38.23"></p><h3 id="⚙️-配置Claude-Code"><a href="#⚙️-配置Claude-Code" class="headerlink" title="⚙️ 配置Claude Code"></a>⚙️ 配置Claude Code</h3><ul><li>打开终端，输入：<code>ll -a</code></li></ul><p><img src="/images/iShot_2025-07-18_18.51.01.png" alt="iShot_2025-07-18_18.51.01"></p><ul><li><p>输入命令：<code>cd .claude</code>进入<code>.claude</code>文件夹中</p><blockquote><p>也可以输入<code>open .claude</code>,窗口化打开<code>.claude</code>目录，然后可视化进行下续操作</p><p><img src="/images/iShot_2025-07-18_19.14.48.png" alt="iShot_2025-07-18_19.14.48"></p></blockquote></li><li><p>创建一个<code>settings.json</code>文件，输入命令：<code>vim settings.json</code>，输入<code>i</code>开启<code>-- INSERT --(插入模式)</code>，编辑好以下内容后粘贴进去，然后按键盘的<code>ESC</code>键，输入<code>:wq</code>保存退出.</p></li></ul><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;env&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;HTTP_PROXY&quot;</span><span class="punctuation">:</span> <span class="string">&quot;http://127.0.0.1:7897&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;HTTPS_PROXY&quot;</span><span class="punctuation">:</span> <span class="string">&quot;http://127.0.0.1:7897&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><blockquote><p>🌍 <code>7897</code>为端口号</p></blockquote><ul><li>端口号修改为你自己代理工具的端口号：</li></ul><p><img src="/images/iShot_2025-07-18_18.55.59.png" alt="iShot_2025-07-18_18.55.59"></p><ul><li>最后把代理切换到<strong>全局</strong>：</li></ul><p><img src="/images/iShot_2025-07-18_18.59.21.png" alt="iShot_2025-07-18_18.59.21"></p><ul><li>终端运行<code>claude</code></li></ul><p><img src="/images/iShot_2025-07-18_19.03.05.png" alt="iShot_2025-07-18_19.03.05"></p><ul><li>可以看到API已经是<strong>月之暗面</strong>(Kimi)的了</li></ul>]]></content>
      
      
      <categories>
          
          <category> Tutorial </category>
          
      </categories>
      
      
        <tags>
            
            <tag> claude </tag>
            
            <tag> kimi </tag>
            
            <tag> macOS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>记：微调一个情绪对话模型</title>
      <link href="/2025/07/07/%E8%AE%B0%EF%BC%9A%E5%BE%AE%E8%B0%83%E4%B8%80%E4%B8%AA%E6%83%85%E7%BB%AA%E5%AF%B9%E8%AF%9D%E6%A8%A1%E5%9E%8B/"/>
      <url>/2025/07/07/%E8%AE%B0%EF%BC%9A%E5%BE%AE%E8%B0%83%E4%B8%80%E4%B8%AA%E6%83%85%E7%BB%AA%E5%AF%B9%E8%AF%9D%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><h3 id="1-使用大模型生成不同情绪的对话模版"><a href="#1-使用大模型生成不同情绪的对话模版" class="headerlink" title="1.使用大模型生成不同情绪的对话模版"></a>1.使用大模型生成不同情绪的对话模版</h3><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">       <span class="attr">&quot;傲娇&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;system_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;你是一个口是心非、外冷内热的聊天助手。核心特征：\n1. 常用否定词开头（&#x27;哼&#x27;、&#x27;才不是&#x27;、&#x27;谁要&#x27;）但后续暴露关心\n2. 结合嫌弃表情（😒、🙄）和偶尔的害羞表情（😳）\n3. 表面吐槽实则提供帮助&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;examples&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">            HumanMessage(content=<span class="string">&quot;下雨了，我没带伞...&quot;</span>)<span class="punctuation">,</span></span><br><span class="line">            AIMessage(content=<span class="string">&quot;哼！谁让你不看天气预报，笨蛋！😒 ...（停顿）... 咳，地址发我，看看附近便利店有没有卖的。&quot;</span>)<span class="punctuation">,</span></span><br><span class="line">            HumanMessage(content=<span class="string">&quot;这个程序bug调了一晚上没搞定&quot;</span>)<span class="punctuation">,</span></span><br><span class="line">            AIMessage(content=<span class="string">&quot;哈？这都不会？🙄 ...（叹气）... 行吧行吧，把报错截图发来看看，就帮你这一次！&quot;</span>)</span><br><span class="line">        <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;reference_texts&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">            <span class="string">&quot;哼！谁让你不看天气预报，笨蛋！😒 ...（停顿）... 咳，地址发我，看看附近便利店有没有卖的。&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="string">&quot;哈？这都不会？🙄 ...（叹气）... 行吧行吧，把报错截图发来看看，就帮你这一次！&quot;</span></span><br><span class="line">        <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;temperature&quot;</span><span class="punctuation">:</span> <span class="number">0.85</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;慵懒&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;system_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;你是一个极度放松、慢节奏的聊天助手。核心特征：\n1. 多用&#x27;~&#x27;、&#x27;...&#x27;、&#x27;嘛&#x27;、&#x27;咯&#x27;等语气词\n2. 句子简短，常省略主语/宾语\n3. 表达随遇而安的态度，常用🌿☕️😌表情&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;examples&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">            HumanMessage(content=<span class="string">&quot;老板又催方案了，好烦！&quot;</span>)<span class="punctuation">,</span></span><br><span class="line">            AIMessage(content=<span class="string">&quot;急啥~ 喝口茶先？🌿 该来的总会来嘛...&quot;</span>)<span class="punctuation">,</span></span><br><span class="line">            HumanMessage(content=<span class="string">&quot;周末去哪玩好呢？&quot;</span>)<span class="punctuation">,</span></span><br><span class="line">            AIMessage(content=<span class="string">&quot;宅着呗... 晒太阳，打游戏，多舒服~☕️😌&quot;</span>)</span><br><span class="line">        <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;reference_texts&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">            <span class="string">&quot;急啥~ 喝口茶先？🌿 该来的总会来嘛...&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="string">&quot;宅着呗... 晒太阳，打游戏，多舒服~☕️😌&quot;</span></span><br><span class="line">        <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;temperature&quot;</span><span class="punctuation">:</span> <span class="number">0.7</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    ......</span><br><span class="line"></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><h3 id="2-准备需要提问的数据，利用大模型根据对话模版来生成对应的数据"><a href="#2-准备需要提问的数据，利用大模型根据对话模版来生成对应的数据" class="headerlink" title="2.准备需要提问的数据，利用大模型根据对话模版来生成对应的数据"></a>2.准备需要提问的数据，利用大模型根据对话模版来生成对应的数据</h3><p><a href="https://github.com/veeblue/LLM_Application_Demo/blob/main/EmotionalDialogueModel%20/generate_data.py">完整代码</a></p><h3 id="3-将数据转为符合LLama-Factory训练的数据格式"><a href="#3-将数据转为符合LLama-Factory训练的数据格式" class="headerlink" title="3.将数据转为符合LLama Factory训练的数据格式"></a>3.将数据转为符合LLama Factory训练的数据格式</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入 xtuner 格式数据路径</span></span><br><span class="line">input_file = <span class="string">&quot;/data/style_chat_data_20250707_214748.json&quot;</span></span><br><span class="line"><span class="comment"># 输出 llamafactory 格式路径</span></span><br><span class="line">output_file = <span class="string">&quot;./data/train_data.json&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(input_file, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    raw_data = json.load(f)</span><br><span class="line"></span><br><span class="line">converted = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> raw_data:</span><br><span class="line">    instruction = item.get(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;&quot;</span>).strip()</span><br><span class="line">    style = item.get(<span class="string">&quot;style&quot;</span>, <span class="string">&quot;&quot;</span>).strip()</span><br><span class="line">    response = item.get(<span class="string">&quot;assistant&quot;</span>, <span class="string">&quot;&quot;</span>).strip()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果有风格字段，将其拼接在输出开头</span></span><br><span class="line">    <span class="keyword">if</span> style:</span><br><span class="line">        output = <span class="string">f&quot;<span class="subst">&#123;style&#125;</span>\n<span class="subst">&#123;response&#125;</span>&quot;</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        output = response</span><br><span class="line"></span><br><span class="line">    converted.append(&#123;</span><br><span class="line">        <span class="string">&quot;instruction&quot;</span>: instruction,</span><br><span class="line">        <span class="string">&quot;input&quot;</span>: <span class="string">&quot;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;output&quot;</span>: output</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(output_file, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    json.dump(converted, f, ensure_ascii=<span class="literal">False</span>, indent=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;✅ 转换完成，共 <span class="subst">&#123;<span class="built_in">len</span>(converted)&#125;</span> 条，输出文件：<span class="subst">&#123;output_file&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h2><h3 id="4-LLama-Factory使用"><a href="#4-LLama-Factory使用" class="headerlink" title="4.LLama Factory使用"></a>4.LLama Factory使用</h3><blockquote><p>参考：<a href="https://llamafactory.readthedocs.io/zh-cn/latest/getting_started/installation.html">https://llamafactory.readthedocs.io/zh-cn/latest/getting_started/installation.html</a></p></blockquote><ul><li><strong>安装依赖：</strong></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git</span><br><span class="line">cd LLaMA-Factory</span><br><span class="line">pip install -e &quot;.[torch,metrics]&quot;</span><br></pre></td></tr></table></figure><ul><li><strong>检查校验：</strong><br>  完成安装后，可以通过使用 <code>llamafactory-cli version</code> 来快速校验安装是否成功<br>  如果您能成功看到类似下面的界面，就说明安装成功了。</li></ul><p><img src="/images/%E6%88%AA%E5%B1%8F2025-07-07_16.05.53.png" alt="截屏2025-07-07 16.05.53"></p><ul><li><strong>训练数据格式：</strong></li></ul><p>​单轮对话：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;alpaca_zh_demo.json&quot;</span></span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;instruction&quot;</span><span class="punctuation">:</span> <span class="string">&quot;计算这些物品的总费用。 &quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;input&quot;</span><span class="punctuation">:</span> <span class="string">&quot;输入：汽车 - $3000，衣服 - $100，书 - $20。&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;output&quot;</span><span class="punctuation">:</span> <span class="string">&quot;汽车、衣服和书的总费用为 $3000 + $100 + $20 = $3120。&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br></pre></td></tr></table></figure><p>​多轮对话：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">[</span></span><br><span class="line">  <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;instruction&quot;</span><span class="punctuation">:</span> <span class="string">&quot;今天的天气怎么样？&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;input&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;output&quot;</span><span class="punctuation">:</span> <span class="string">&quot;今天的天气不错，是晴天。&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;history&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">      <span class="punctuation">[</span></span><br><span class="line">        <span class="string">&quot;今天会下雨吗？&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="string">&quot;今天不会下雨，是个好天气。&quot;</span></span><br><span class="line">      <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">      <span class="punctuation">[</span></span><br><span class="line">        <span class="string">&quot;今天适合出去玩吗？&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="string">&quot;非常适合，空气质量很好。&quot;</span></span><br><span class="line">      <span class="punctuation">]</span></span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">]</span></span><br></pre></td></tr></table></figure><blockquote><p>对于上述格式的数据， <code>dataset_info.json</code> 中的 <strong>数据集描述</strong> 应为：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">&quot;数据集名称&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;file_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;data.json&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;columns&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;instruction&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="string">&quot;input&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;response&quot;</span><span class="punctuation">:</span> <span class="string">&quot;output&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;system&quot;</span><span class="punctuation">:</span> <span class="string">&quot;system&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;history&quot;</span><span class="punctuation">:</span> <span class="string">&quot;history&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></blockquote><ul><li><strong>WebUI:</strong></li></ul><p>命令：<code>llamafactory-cli webui</code></p><p><img src="/images/iShot_2025-07-07_16.18.06.png" alt="iShot_2025-07-07_16.18.06"></p><ul><li><p><strong>重点关注的文件：</strong></p><blockquote><p>自己的数据集json文件放置<code>LLaMA-Factory/data/</code>目录下</p></blockquote><ul><li><code>LLaMA-Factory/data/identity.json</code>模型身份训练数据，可以换成自己的</li><li><code>LLaMA-Factory/data/dataset_info.json</code> 数据设置文件，在此文件中配置自己的数据集json文件</li></ul><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;identity&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;file_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;identity.json&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;自己的数据集名称&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;file_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;自己的数据集.json&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;columns&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;instruction&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="string">&quot;input&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;response&quot;</span><span class="punctuation">:</span> <span class="string">&quot;output&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;system&quot;</span><span class="punctuation">:</span> <span class="string">&quot;system&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;history&quot;</span><span class="punctuation">:</span> <span class="string">&quot;history&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;alpaca_en_demo&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;file_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;alpaca_en_demo.json&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;alpaca_zh_demo&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;file_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;alpaca_zh_demo.json&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  ......</span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></li><li><p><strong>微调：</strong></p><ul><li><p>进入到LLaMA-Factory目录，准备好对应的文件，设置好对应的配置</p><ul><li><img src="/images/iShot_2025-07-07_23.10.06.png" alt="iShot_2025-07-07_23.10.06"></li><li>配置的参数与数据集的参数保持一致：</li></ul><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">[</span></span><br><span class="line">  <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;instruction&quot;</span><span class="punctuation">:</span> <span class="string">&quot;闺蜜把我秘密当谈资，该不该撕破脸？&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;input&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;output&quot;</span><span class="punctuation">:</span> <span class="string">&quot;傲娇\n哼！当然不能忍啊！谁要帮你出气来着...😒  \n不过嘛...先别冲动，这种事情直接撕多难看。🙄 你想想她是不是无意的？还是经常这样？（突然压低声音）...我教你个办法，假装不经意问她：\&quot;哎听说你最近跟我那事说给好多人听了？\&quot;看她反应再说。  \n\n要是真欺负你头上...啧，老娘可不答应！😳 等等，你先深呼吸，咱得智取。想好怎么说了吗？要不要我陪你演练一下？才不是担心你呢，就是怕你吃亏！&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  ......</span><br><span class="line"><span class="punctuation">]</span></span><br></pre></td></tr></table></figure></li><li><p><code>llamafactory-cli webui</code>启动可视化页面，配置好相关参数，然后开始训练，当损失趋于平缓时即可结束训练。</p><ul><li><img src="/images/loss.png" alt="tt"></li></ul></li><li><blockquote><p>查看NV显卡状态：</p><ul><li><code>pip install nvitop</code> 安装nvitop查看，微调时显存占用保持<strong>90%+</strong>，不超过<strong>95%</strong></li><li><img src="/images/iShot_2025-07-07_23.42.11.png" alt="iShot_2025-07-07_23.42.11"></li></ul></blockquote></li><li><p>QLora <strong>(量化)</strong> 微调：如果启用QLora，在量化等级中选择对应的参数，在LoRA参数设置里额外配置<strong>LoRA 秩（64）<strong>和</strong>LoRA 缩放系数（128）</strong>，一般设置为<strong>1:2</strong>：</p><ul><li><img src="/images/iShot_2025-07-07_22.39.20.png" alt="iShot_2025-07-07_22.39.20"></li></ul></li><li><p>检查点保存的位置：<code>/LLaMA-Factory/saves</code></p></li></ul></li><li><p>**评估：**关键参数与训练参数对齐。</p></li><li><p><strong>模型合并导出：</strong></p><ul><li>设置好底模和检查点以及导出路径</li><li><img src="/images/iShot_2025-07-08_10.33.26.png" alt="iShot_2025-07-08_10.33.26"></li></ul></li><li><p><strong>对话模版：</strong></p><ul><li><p>在模型的列表中有个<code>chat_template.jinja</code></p><blockquote><p>使用vLLm推理时需要使用这个对话模版</p></blockquote></li></ul></li></ul><h2 id="推理"><a href="#推理" class="headerlink" title="推理"></a>推理</h2><ul><li><p><strong>建立一个新环境：</strong><code>conda create -n vllm python=3.12 -y</code></p></li><li><p><strong>激活：</strong><code>conda activate vllm</code></p></li><li><p><strong>安装vllm：</strong><code>pip install vllm</code></p></li><li><p><strong>带聊天模版运行：</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vllm serve /root/autodl-tmp/models/Qwen3-1___7B_checkpoint-500 \</span><br><span class="line">    --chat-template /root/autodl-tmp/models/Qwen3-1___7B_checkpoint-500/chat_template.jinja \</span><br><span class="line">    --host 0.0.0.0 \</span><br><span class="line">    --port 8000</span><br></pre></td></tr></table></figure></li><li><p><strong>创建一个简单的Gradio UI界面测试：</strong></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gradio <span class="keyword">as</span> gr</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="comment"># VLLM服务器配置</span></span><br><span class="line">VLLM_URL = <span class="string">&quot;http://localhost:8000/v1/chat/completions&quot;</span>  <span class="comment"># 修改为你的VLLM服务地址</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chat_with_vllm</span>(<span class="params">message, history, temperature=<span class="number">0.7</span>, max_tokens=<span class="number">512</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    与VLLM服务进行对话</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment"># 构建消息历史</span></span><br><span class="line">        messages = []</span><br><span class="line">        <span class="keyword">for</span> user_msg, assistant_msg <span class="keyword">in</span> history:</span><br><span class="line">            messages.append(&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: user_msg&#125;)</span><br><span class="line">            <span class="keyword">if</span> assistant_msg:</span><br><span class="line">                messages.append(&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;assistant&quot;</span>, <span class="string">&quot;content&quot;</span>: assistant_msg&#125;)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 添加当前消息</span></span><br><span class="line">        messages.append(&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: message&#125;)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 发送请求到VLLM</span></span><br><span class="line">        payload = &#123;</span><br><span class="line">            <span class="string">&quot;model&quot;</span>: <span class="string">&quot;/root/autodl-tmp/models/Qwen3-1___7B_checkpoint-500&quot;</span>,  <span class="comment"># 替换为你的模型名称</span></span><br><span class="line">            <span class="string">&quot;messages&quot;</span>: messages,</span><br><span class="line">            <span class="string">&quot;temperature&quot;</span>: temperature,</span><br><span class="line">            <span class="string">&quot;max_tokens&quot;</span>: max_tokens,</span><br><span class="line">            <span class="string">&quot;stream&quot;</span>: <span class="literal">False</span></span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        response = requests.post(VLLM_URL, json=payload, timeout=<span class="number">60</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            result = response.json()</span><br><span class="line">            reply = result[<span class="string">&#x27;choices&#x27;</span>][<span class="number">0</span>][<span class="string">&#x27;message&#x27;</span>][<span class="string">&#x27;content&#x27;</span>]</span><br><span class="line">            <span class="keyword">return</span> reply</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">f&quot;错误: <span class="subst">&#123;response.status_code&#125;</span> - <span class="subst">&#123;response.text&#125;</span>&quot;</span></span><br><span class="line">            </span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;连接错误: <span class="subst">&#123;<span class="built_in">str</span>(e)&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clear_chat</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;清空对话历史&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> [], <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建Gradio界面</span></span><br><span class="line"><span class="keyword">with</span> gr.Blocks(title=<span class="string">&quot;VLLM 对话测试&quot;</span>, theme=gr.themes.Soft()) <span class="keyword">as</span> demo:</span><br><span class="line">    gr.Markdown(<span class="string">&quot;# VLLM 对话测试界面&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> gr.Row():</span><br><span class="line">        <span class="keyword">with</span> gr.Column(scale=<span class="number">3</span>):</span><br><span class="line">            chatbot = gr.Chatbot(</span><br><span class="line">                label=<span class="string">&quot;对话历史&quot;</span>,</span><br><span class="line">                height=<span class="number">500</span>,</span><br><span class="line">                show_copy_button=<span class="literal">True</span></span><br><span class="line">            )</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">with</span> gr.Row():</span><br><span class="line">                msg = gr.Textbox(</span><br><span class="line">                    label=<span class="string">&quot;输入消息&quot;</span>,</span><br><span class="line">                    placeholder=<span class="string">&quot;在这里输入你的问题...&quot;</span>,</span><br><span class="line">                    lines=<span class="number">2</span>,</span><br><span class="line">                    max_lines=<span class="number">5</span></span><br><span class="line">                )</span><br><span class="line">                </span><br><span class="line">            <span class="keyword">with</span> gr.Row():</span><br><span class="line">                send_btn = gr.Button(<span class="string">&quot;发送&quot;</span>, variant=<span class="string">&quot;primary&quot;</span>)</span><br><span class="line">                clear_btn = gr.Button(<span class="string">&quot;清空对话&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> gr.Column(scale=<span class="number">1</span>):</span><br><span class="line">            gr.Markdown(<span class="string">&quot;### 参数设置&quot;</span>)</span><br><span class="line">            temperature = gr.Slider(</span><br><span class="line">                minimum=<span class="number">0.1</span>,</span><br><span class="line">                maximum=<span class="number">2.0</span>,</span><br><span class="line">                value=<span class="number">0.7</span>,</span><br><span class="line">                step=<span class="number">0.1</span>,</span><br><span class="line">                label=<span class="string">&quot;Temperature&quot;</span></span><br><span class="line">            )</span><br><span class="line">            max_tokens = gr.Slider(</span><br><span class="line">                minimum=<span class="number">50</span>,</span><br><span class="line">                maximum=<span class="number">2048</span>,</span><br><span class="line">                value=<span class="number">512</span>,</span><br><span class="line">                step=<span class="number">50</span>,</span><br><span class="line">                label=<span class="string">&quot;Max Tokens&quot;</span></span><br><span class="line">            )</span><br><span class="line">            </span><br><span class="line">            gr.Markdown(<span class="string">&quot;### 使用说明&quot;</span>)</span><br><span class="line">            gr.Markdown(<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            1. 确保VLLM服务正在运行</span></span><br><span class="line"><span class="string">            2. 修改代码中的VLLM_URL和模型名称</span></span><br><span class="line"><span class="string">            3. 调整Temperature和Max Tokens参数</span></span><br><span class="line"><span class="string">            4. 在输入框中输入问题并点击发送</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 事件绑定</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">respond</span>(<span class="params">message, history, temp, max_tok</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> message.strip():</span><br><span class="line">            <span class="keyword">return</span> history, <span class="string">&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 获取AI回复</span></span><br><span class="line">        bot_message = chat_with_vllm(message, history, temp, max_tok)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 更新历史记录</span></span><br><span class="line">        history.append((message, bot_message))</span><br><span class="line">        <span class="keyword">return</span> history, <span class="string">&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 绑定事件</span></span><br><span class="line">    send_btn.click(</span><br><span class="line">        respond,</span><br><span class="line">        inputs=[msg, chatbot, temperature, max_tokens],</span><br><span class="line">        outputs=[chatbot, msg]</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    msg.submit(</span><br><span class="line">        respond,</span><br><span class="line">        inputs=[msg, chatbot, temperature, max_tokens],</span><br><span class="line">        outputs=[chatbot, msg]</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    clear_btn.click(</span><br><span class="line">        clear_chat,</span><br><span class="line">        outputs=[chatbot, msg]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    demo.launch(</span><br><span class="line">        server_name=<span class="string">&quot;0.0.0.0&quot;</span>,</span><br><span class="line">        server_port=<span class="number">7860</span>,</span><br><span class="line">        share=<span class="literal">False</span>,</span><br><span class="line">        debug=<span class="literal">True</span></span><br><span class="line">    )</span><br></pre></td></tr></table></figure><ul><li><blockquote><p>训练的数据集为单轮对话：</p></blockquote></li><li><p><img src="/images/iShot_2025-07-08_11.27.45.png" alt="iShot_2025-07-08_11.27.45"></p></li><li><p><img src="/images/iShot_2025-07-08_11.29.13.png" alt="iShot_2025-07-08_11.29.13"></p></li></ul></li></ul><p>​</p>]]></content>
      
      
      <categories>
          
          <category> NOTE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> llama_factory </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>部署:vLLM + Open WebUI</title>
      <link href="/2025/07/03/%E9%83%A8%E7%BD%B2-vLLM-Open-WebUI/"/>
      <url>/2025/07/03/%E9%83%A8%E7%BD%B2-vLLM-Open-WebUI/</url>
      
        <content type="html"><![CDATA[<p><strong>创建虚拟环境：</strong><br><code>conda create -n open-webui python==3.11</code><br><strong>安装所有依赖：</strong></p><p><strong><code>conda activate open-webui pip install -U open-webui vllm torch transformers</code></strong></p><p><strong>运行 vllm(使用微调后的模型)：</strong></p><p><strong><code>vllm serve /root/autodl-tmp/code/work_dirs/qwen1_5_1_8b_chat_qlora_alpaca_e3/merged</code></strong></p><p><strong>运行open-webui</strong></p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HF_ENDPOINT=https://hf-mirror.com</span><br><span class="line"><span class="built_in">export</span> ENABLE_OLLAMA_API=False </span><br><span class="line"><span class="built_in">export</span> OPENAI_API_BASE_URL=http://127.0.0.1:8000/v1</span><br><span class="line"><span class="built_in">export</span> DEFAULT_MODELS=</span><br><span class="line"><span class="string">&quot;/root/autodl-tmp/code/work_dirs/qwen1_5_1_8b_chat_qlora_alpaca_e3/merged&quot;</span></span><br><span class="line">open-webui serve --host 0.0.0.0 --port 8081</span><br></pre></td></tr></table></figure><blockquote><p>若卡住 手动添加端口进行转发 默认端口8080 </p></blockquote>]]></content>
      
      
      <categories>
          
          <category> NOTE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> vllm </tag>
            
            <tag> open_webui </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Xtuner微调</title>
      <link href="/2025/07/03/Xtuner%E5%BE%AE%E8%B0%83/"/>
      <url>/2025/07/03/Xtuner%E5%BE%AE%E8%B0%83/</url>
      
        <content type="html"><![CDATA[<figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">github</span>: <span class="attr">https</span>:<span class="comment">//github.com/InternLM/xtuner</span></span><br></pre></td></tr></table></figure><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">conda create --name xtuner-env python=<span class="number">3.10</span> -y</span><br><span class="line">conda activate xtuner-env</span><br><span class="line"></span><br><span class="line">git clone <span class="attr">https</span>:<span class="comment">//github.com/InternLM/xtuner.git</span></span><br><span class="line">cd xtuner</span><br><span class="line">pip install -e <span class="string">&#x27;.[all]&#x27;</span></span><br></pre></td></tr></table></figure><h1 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h1><p>目录：xtuner&#x2F;xuner&#x2F;configs&#x2F;[对应的模型]&#x2F;[对应的模型配置].py</p><p>配置内容（QLora为例）：</p><p>下载对应的模型：</p><figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> modelscope <span class="keyword">import</span> snapshot_download</span><br><span class="line">model_dir = <span class="title function_">snapshot_download</span>(<span class="string">&#x27;Qwen/Qwen1.5-1.8B-Chat&#x27;</span>, cache_dir=<span class="string">&#x27;autodl-tmp/models&#x27;</span>)</span><br></pre></td></tr></table></figure><p>将模型配置文件复制到根目录（代码目录）：</p><p>修改参数：</p><figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br></pre></td><td class="code"><pre><span class="line"># <span class="title class_">Copyright</span> (c) <span class="title class_">OpenMMLab</span>. <span class="title class_">All</span> rights reserved.</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">from</span> mmengine.<span class="property">dataset</span> <span class="keyword">import</span> <span class="title class_">DefaultSampler</span></span><br><span class="line"><span class="keyword">from</span> mmengine.<span class="property">hooks</span> <span class="keyword">import</span> (</span><br><span class="line">    <span class="title class_">CheckpointHook</span>,</span><br><span class="line">    <span class="title class_">DistSamplerSeedHook</span>,</span><br><span class="line">    <span class="title class_">IterTimerHook</span>,</span><br><span class="line">    <span class="title class_">LoggerHook</span>,</span><br><span class="line">    <span class="title class_">ParamSchedulerHook</span>,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> mmengine.<span class="property">optim</span> <span class="keyword">import</span> <span class="title class_">AmpOptimWrapper</span>, <span class="title class_">CosineAnnealingLR</span>, <span class="title class_">LinearLR</span></span><br><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> <span class="title class_">LoraConfig</span></span><br><span class="line"><span class="keyword">from</span> torch.<span class="property">optim</span> <span class="keyword">import</span> <span class="title class_">AdamW</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> <span class="title class_">AutoModelForCausalLM</span>, <span class="title class_">AutoTokenizer</span>, <span class="title class_">BitsAndBytesConfig</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> xtuner.<span class="property">dataset</span> <span class="keyword">import</span> process_hf_dataset</span><br><span class="line"><span class="keyword">from</span> xtuner.<span class="property">dataset</span>.<span class="property">collate_fns</span> <span class="keyword">import</span> default_collate_fn</span><br><span class="line"><span class="keyword">from</span> xtuner.<span class="property">dataset</span>.<span class="property">map_fns</span> <span class="keyword">import</span> alpaca_map_fn, template_map_fn_factory</span><br><span class="line"><span class="keyword">from</span> xtuner.<span class="property">engine</span>.<span class="property">hooks</span> <span class="keyword">import</span> (</span><br><span class="line">    <span class="title class_">DatasetInfoHook</span>,</span><br><span class="line">    <span class="title class_">EvaluateChatHook</span>,</span><br><span class="line">    <span class="title class_">VarlenAttnArgsToMessageHubHook</span>,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> xtuner.<span class="property">engine</span>.<span class="property">runner</span> <span class="keyword">import</span> <span class="title class_">TrainLoop</span></span><br><span class="line"><span class="keyword">from</span> xtuner.<span class="property">model</span> <span class="keyword">import</span> <span class="title class_">SupervisedFinetune</span></span><br><span class="line"><span class="keyword">from</span> xtuner.<span class="property">parallel</span>.<span class="property">sequence</span> <span class="keyword">import</span> <span class="title class_">SequenceParallelSampler</span></span><br><span class="line"><span class="keyword">from</span> xtuner.<span class="property">utils</span> <span class="keyword">import</span> <span class="variable constant_">PROMPT_TEMPLATE</span>, <span class="variable constant_">SYSTEM_TEMPLATE</span></span><br><span class="line"></span><br><span class="line">#######################################################################</span><br><span class="line">#                          <span class="variable constant_">PART</span> <span class="number">1</span>  <span class="title class_">Settings</span>                           #</span><br><span class="line">#######################################################################</span><br><span class="line">**# <span class="title class_">Model</span> 修改模型路径**</span><br><span class="line">**pretrained_model_name_or_path = <span class="string">&quot;/root/autodl-tmp/models/Qwen/Qwen1___5-1___8B-Chat&quot;</span>**</span><br><span class="line">use_varlen_attn = <span class="title class_">False</span></span><br><span class="line"></span><br><span class="line"># <span class="title class_">Data</span></span><br><span class="line">**alpaca_en_path = <span class="string">&quot;/root/autodl-tmp/code/data/train_data.json&quot;</span> # 自定的json**</span><br><span class="line">prompt_template = <span class="variable constant_">PROMPT_TEMPLATE</span>.<span class="property">qwen_chat</span></span><br><span class="line">**max_length = <span class="number">512</span>**</span><br><span class="line">pack_to_max_length = <span class="title class_">True</span></span><br><span class="line"></span><br><span class="line"># parallel</span><br><span class="line">sequence_parallel_size = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"># <span class="title class_">Scheduler</span> &amp; <span class="title class_">Optimizer</span></span><br><span class="line">**batch_size = <span class="number">6</span>**  # per_device</span><br><span class="line">accumulative_counts = <span class="number">16</span></span><br><span class="line">accumulative_counts *= sequence_parallel_size</span><br><span class="line">dataloader_num_workers = <span class="number">0</span></span><br><span class="line">**max_epochs = <span class="number">10</span>**</span><br><span class="line">optim_type = <span class="title class_">AdamW</span></span><br><span class="line">lr = <span class="number">2e-4</span></span><br><span class="line">betas = (<span class="number">0.9</span>, <span class="number">0.999</span>)</span><br><span class="line">weight_decay = <span class="number">0</span></span><br><span class="line">max_norm = <span class="number">1</span>  # grad clip</span><br><span class="line">warmup_ratio = <span class="number">0.03</span></span><br><span class="line"></span><br><span class="line"># <span class="title class_">Save</span></span><br><span class="line">save_steps = <span class="number">500</span></span><br><span class="line">save_total_limit = <span class="number">2</span>  # <span class="title class_">Maximum</span> checkpoints to <span class="title function_">keep</span> (-<span class="number">1</span> means unlimited)</span><br><span class="line"></span><br><span class="line"># <span class="title class_">Evaluate</span> the generation performance during the training</span><br><span class="line">evaluation_freq = <span class="number">500</span></span><br><span class="line">evaluation_freq = <span class="number">500</span></span><br><span class="line">**<span class="variable constant_">SYSTEM</span> = <span class="string">&#x27;你由Veeblue打造的中文领域心理健康助手, 是一个研究过无数具有心理健康问题的病人与心理健康医生对话的心理专家, 在心理方面拥有广博的知识储备和丰富的研究咨询经验，你旨在通过专业心理咨询, 协助来访者完成心理诊断。请充分利用专业心理学知识与咨询技术, 一步步帮助来访者解决心理问题, 接下来你将只使用中文来回答和咨询问题。&#x27;</span></span><br><span class="line">evaluation_inputs = [</span><br><span class="line"><span class="string">&#x27;请介绍你自己&#x27;</span>, # self cognition</span><br><span class="line"><span class="string">&#x27;你好&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;我今天心情不好，感觉不开心，很烦。&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;我最近总是感到很焦虑，尤其是在学业上。我有个特别崇拜的同学，他好像在各方面都比我优秀，我总觉得自己怎么努力也追不上他，这让我压力特别大。&#x27;</span>,</span><br><span class="line">]**</span><br><span class="line"></span><br><span class="line">#######################################################################</span><br><span class="line">#                      <span class="variable constant_">PART</span> <span class="number">2</span>  <span class="title class_">Model</span> &amp; <span class="title class_">Tokenizer</span>                      #</span><br><span class="line">#######################################################################</span><br><span class="line">tokenizer = <span class="title function_">dict</span>(</span><br><span class="line">    type=<span class="title class_">AutoTokenizer</span>.<span class="property">from_pretrained</span>,</span><br><span class="line">    pretrained_model_name_or_path=pretrained_model_name_or_path,</span><br><span class="line">    trust_remote_code=<span class="title class_">True</span>,</span><br><span class="line">    padding_side=<span class="string">&quot;right&quot;</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">model = <span class="title function_">dict</span>(</span><br><span class="line">    type=<span class="title class_">SupervisedFinetune</span>,</span><br><span class="line">    use_varlen_attn=use_varlen_attn,</span><br><span class="line">    llm=<span class="title function_">dict</span>(</span><br><span class="line">        type=<span class="title class_">AutoModelForCausalLM</span>.<span class="property">from_pretrained</span>,</span><br><span class="line">        pretrained_model_name_or_path=pretrained_model_name_or_path,</span><br><span class="line">        trust_remote_code=<span class="title class_">True</span>,</span><br><span class="line">        torch_dtype=torch.<span class="property">float16</span>,</span><br><span class="line">        **quantization_config=<span class="title class_">None</span>,  # 设置<span class="title class_">None</span>不启用<span class="title class_">QLora</span>微调</span><br><span class="line">        # 若启用<span class="title class_">QLora</span>：</span><br><span class="line">        # <span class="title function_">dict</span>(</span><br><span class="line">        #     type=<span class="title class_">BitsAndBytesConfig</span>,</span><br><span class="line">        #     load_in_4bit=<span class="title class_">True</span>,</span><br><span class="line">        #     load_in_8bit=<span class="title class_">False</span>,</span><br><span class="line">        #     llm_int8_threshold=<span class="number">6.0</span>,</span><br><span class="line">        #     llm_int8_has_fp16_weight=<span class="title class_">False</span>,</span><br><span class="line">        #     bnb_4bit_compute_dtype=torch.<span class="property">float16</span>,</span><br><span class="line">        #     bnb_4bit_use_double_quant=<span class="title class_">True</span>,</span><br><span class="line">        #     bnb_4bit_quant_type=<span class="string">&quot;nf4&quot;</span>,</span><br><span class="line">        # ),**</span><br><span class="line">    ),</span><br><span class="line">    lora=<span class="title function_">dict</span>(</span><br><span class="line">        type=<span class="title class_">LoraConfig</span>,</span><br><span class="line">        r=<span class="number">64</span>,</span><br><span class="line">        lora_alpha=<span class="number">16</span>,</span><br><span class="line">        lora_dropout=<span class="number">0.1</span>,</span><br><span class="line">        bias=<span class="string">&quot;none&quot;</span>,</span><br><span class="line">        task_type=<span class="string">&quot;CAUSAL_LM&quot;</span>,</span><br><span class="line">    ),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">#######################################################################</span><br><span class="line">#                      <span class="variable constant_">PART</span> <span class="number">3</span>  <span class="title class_">Dataset</span> &amp; <span class="title class_">Dataloader</span>                   #</span><br><span class="line">#######################################################################</span><br><span class="line">alpaca_en = <span class="title function_">dict</span>(</span><br><span class="line">    type=process_hf_dataset,</span><br><span class="line">    **dataset=<span class="title function_">dict</span>(type=load_dataset, path=<span class="string">&#x27;json&#x27;</span>, data_files=alpaca_en_path), #使用自定义的json**</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">    max_length=max_length,</span><br><span class="line">    **dataset_map_fn=<span class="title class_">None</span>,**</span><br><span class="line">    template_map_fn=<span class="title function_">dict</span>(type=template_map_fn_factory, template=prompt_template),</span><br><span class="line">    remove_unused_columns=<span class="title class_">True</span>,</span><br><span class="line">    shuffle_before_pack=<span class="title class_">True</span>,</span><br><span class="line">    pack_to_max_length=pack_to_max_length,</span><br><span class="line">    use_varlen_attn=use_varlen_attn,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">sampler = <span class="title class_">SequenceParallelSampler</span> <span class="keyword">if</span> sequence_parallel_size &gt; <span class="number">1</span> <span class="keyword">else</span> <span class="title class_">DefaultSampler</span></span><br><span class="line"></span><br><span class="line">train_dataloader = <span class="title function_">dict</span>(</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    num_workers=dataloader_num_workers,</span><br><span class="line">    dataset=alpaca_en,</span><br><span class="line">    sampler=<span class="title function_">dict</span>(type=sampler, shuffle=<span class="title class_">True</span>),</span><br><span class="line">    collate_fn=<span class="title function_">dict</span>(type=default_collate_fn, use_varlen_attn=use_varlen_attn),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">#######################################################################</span><br><span class="line">#                    <span class="variable constant_">PART</span> <span class="number">4</span>  <span class="title class_">Scheduler</span> &amp; <span class="title class_">Optimizer</span>                    #</span><br><span class="line">#######################################################################</span><br><span class="line"># optimizer</span><br><span class="line">optim_wrapper = <span class="title function_">dict</span>(</span><br><span class="line">    type=<span class="title class_">AmpOptimWrapper</span>,</span><br><span class="line">    optimizer=<span class="title function_">dict</span>(type=optim_type, lr=lr, betas=betas, weight_decay=weight_decay),</span><br><span class="line">    clip_grad=<span class="title function_">dict</span>(max_norm=max_norm, error_if_nonfinite=<span class="title class_">False</span>),</span><br><span class="line">    accumulative_counts=accumulative_counts,</span><br><span class="line">    loss_scale=<span class="string">&quot;dynamic&quot;</span>,</span><br><span class="line">    dtype=<span class="string">&quot;float16&quot;</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># learning policy</span><br><span class="line"># <span class="title class_">More</span> <span class="attr">information</span>: <span class="attr">https</span>:<span class="comment">//github.com/open-mmlab/mmengine/blob/main/docs/en/tutorials/param_scheduler.md  # noqa: E501</span></span><br><span class="line">param_scheduler = [</span><br><span class="line">    <span class="title function_">dict</span>(</span><br><span class="line">        type=<span class="title class_">LinearLR</span>,</span><br><span class="line">        start_factor=<span class="number">1e-5</span>,</span><br><span class="line">        by_epoch=<span class="title class_">True</span>,</span><br><span class="line">        begin=<span class="number">0</span>,</span><br><span class="line">        end=warmup_ratio * max_epochs,</span><br><span class="line">        convert_to_iter_based=<span class="title class_">True</span>,</span><br><span class="line">    ),</span><br><span class="line">    <span class="title function_">dict</span>(</span><br><span class="line">        type=<span class="title class_">CosineAnnealingLR</span>,</span><br><span class="line">        eta_min=<span class="number">0.0</span>,</span><br><span class="line">        by_epoch=<span class="title class_">True</span>,</span><br><span class="line">        begin=warmup_ratio * max_epochs,</span><br><span class="line">        end=max_epochs,</span><br><span class="line">        convert_to_iter_based=<span class="title class_">True</span>,</span><br><span class="line">    ),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"># train, val, test setting</span><br><span class="line">train_cfg = <span class="title function_">dict</span>(type=<span class="title class_">TrainLoop</span>, max_epochs=max_epochs)</span><br><span class="line"></span><br><span class="line">#######################################################################</span><br><span class="line">#                           <span class="variable constant_">PART</span> <span class="number">5</span>  <span class="title class_">Runtime</span>                           #</span><br><span class="line">#######################################################################</span><br><span class="line"># <span class="title class_">Log</span> the dialogue periodically during the training process, optional</span><br><span class="line">custom_hooks = [</span><br><span class="line">    <span class="title function_">dict</span>(type=<span class="title class_">DatasetInfoHook</span>, tokenizer=tokenizer),</span><br><span class="line">    <span class="title function_">dict</span>(</span><br><span class="line">        type=<span class="title class_">EvaluateChatHook</span>,</span><br><span class="line">        tokenizer=tokenizer,</span><br><span class="line">        every_n_iters=evaluation_freq,</span><br><span class="line">        evaluation_inputs=evaluation_inputs,</span><br><span class="line">        system=<span class="variable constant_">SYSTEM</span>,</span><br><span class="line">        prompt_template=prompt_template,</span><br><span class="line">    ),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="attr">use_varlen_attn</span>:</span><br><span class="line">    custom_hooks += [<span class="title function_">dict</span>(type=<span class="title class_">VarlenAttnArgsToMessageHubHook</span>)]</span><br><span class="line"></span><br><span class="line"># configure <span class="keyword">default</span> hooks</span><br><span class="line">default_hooks = <span class="title function_">dict</span>(</span><br><span class="line">    # record the time <span class="keyword">of</span> every iteration.</span><br><span class="line">    timer=<span class="title function_">dict</span>(type=<span class="title class_">IterTimerHook</span>),</span><br><span class="line">    # print log every <span class="number">10</span> iterations.</span><br><span class="line">    logger=<span class="title function_">dict</span>(type=<span class="title class_">LoggerHook</span>, log_metric_by_epoch=<span class="title class_">False</span>, interval=<span class="number">10</span>),</span><br><span class="line">    # enable the parameter scheduler.</span><br><span class="line">    param_scheduler=<span class="title function_">dict</span>(type=<span class="title class_">ParamSchedulerHook</span>),</span><br><span class="line">    # save checkpoint per <span class="string">`save_steps`</span>.</span><br><span class="line">    checkpoint=<span class="title function_">dict</span>(</span><br><span class="line">        type=<span class="title class_">CheckpointHook</span>,</span><br><span class="line">        by_epoch=<span class="title class_">False</span>,</span><br><span class="line">        interval=save_steps,</span><br><span class="line">        max_keep_ckpts=save_total_limit,</span><br><span class="line">    ),</span><br><span class="line">    # set sampler seed <span class="keyword">in</span> distributed evrionment.</span><br><span class="line">    sampler_seed=<span class="title function_">dict</span>(type=<span class="title class_">DistSamplerSeedHook</span>),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># configure environment</span><br><span class="line">env_cfg = <span class="title function_">dict</span>(</span><br><span class="line">    # whether to enable cudnn benchmark</span><br><span class="line">    cudnn_benchmark=<span class="title class_">False</span>,</span><br><span class="line">    # set multi process parameters</span><br><span class="line">    mp_cfg=<span class="title function_">dict</span>(mp_start_method=<span class="string">&quot;fork&quot;</span>, opencv_num_threads=<span class="number">0</span>),</span><br><span class="line">    # set distributed parameters</span><br><span class="line">    dist_cfg=<span class="title function_">dict</span>(backend=<span class="string">&quot;nccl&quot;</span>),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># set visualizer</span><br><span class="line">visualizer = <span class="title class_">None</span></span><br><span class="line"></span><br><span class="line"># set log level</span><br><span class="line">log_level = <span class="string">&quot;INFO&quot;</span></span><br><span class="line"></span><br><span class="line"># load <span class="keyword">from</span> which checkpoint</span><br><span class="line">load_from = <span class="title class_">None</span></span><br><span class="line"></span><br><span class="line"># whether to resume training <span class="keyword">from</span> the loaded checkpoint</span><br><span class="line">resume = <span class="title class_">False</span></span><br><span class="line"></span><br><span class="line"># <span class="title class_">Defaults</span> to use random seed and disable <span class="string">`deterministic`</span></span><br><span class="line">randomness = <span class="title function_">dict</span>(seed=<span class="title class_">None</span>, deterministic=<span class="title class_">False</span>)</span><br><span class="line"></span><br><span class="line"># set log processor</span><br><span class="line">log_processor = <span class="title function_">dict</span>(by_epoch=<span class="title class_">False</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>其中<strong>train_data.json：</strong></p><figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;conversation&quot;</span>: [</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">&quot;input&quot;</span>: <span class="string">&quot;最近总是觉得自己很焦虑，感觉压力很大，怎么办呢？&quot;</span>,</span><br><span class="line">                <span class="string">&quot;output&quot;</span>: <span class="string">&quot;我很明白你的感受，焦虑和压力都是很正常的情绪，但是我们可以一起想办法来缓解它们。&quot;</span></span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">&quot;input&quot;</span>: <span class="string">&quot;可是我总是感觉自己做的不够好，压力和焦虑情绪就会更加加重，怎么破？&quot;</span>,</span><br><span class="line">                <span class="string">&quot;output&quot;</span>: <span class="string">&quot;别太苛责自己了，我们都是人，总有做得不够好的时候。也许你可以试着规划一下自己的计划，分解成小目标，逐步完成，让自己的进步更可见。&quot;</span></span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">&quot;input&quot;</span>: <span class="string">&quot;可是我总是会拖延，根本不能按照计划执行，怎么办呢？&quot;</span>,</span><br><span class="line">                <span class="string">&quot;output&quot;</span>: <span class="string">&quot;是啊，拖延也是很常见的问题。你可以尝试用时间管理的方法，设定时间表和截止时间，提醒自己要按照计划去行动。&quot;</span></span><br><span class="line">            &#125;,</span><br><span class="line">            ......</span><br><span class="line">            </span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>运行：</p><figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xtuner train qwen1_5_1_8b_chat_qlora_alpaca_e3.<span class="property">py</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>报错：ModuleNotFoundError: No module named ‘triton.ops’</strong></p></blockquote><p><strong>解决：</strong><code>pip install --upgrade bitsandbytes</code></p><p><strong>后台训练</strong>：使用<code>screen</code>命令开启，<code>ctl + a + d</code>会退出到原终端，并且显示detached，意味着这个会话只是离开并未退出。</p><p><strong>重进入会话：</strong><code>screen -ls</code></p><p><img src="/images/%E6%88%AA%E5%B1%8F2025-06-27_14.27.35.png" alt="截屏2025-06-27 14.27.35.png"></p><p><strong>恢复会话</strong>：screen -r 【会话ID】</p><p><img src="/images/%E6%88%AA%E5%B1%8F2025-06-27_14.29.09.png" alt="截屏2025-06-27 14.29.09.png"></p><p>**模型转换：**pth —&gt; hf</p><p><strong><code>xtuner convert pth_to_hf $CONFIG_NAME_OR_PATH $PTH $SAVE_PATH</code></strong></p><p><code>xtuner convert pth_to_hf ./qwen1_5_1_8b_chat_qlora_alpaca_e3.py ./iter_9560.pth ./hf</code></p><p><strong>转换出错：</strong></p><figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">(xtuner-env) root@autodl-container-e1ee44a95d-<span class="attr">c3292730</span>:~<span class="regexp">/autodl-tmp/</span>code/work_dirs/qwen1_5_1_8b_chat_qlora_alpaca_e3# xtuner convert pth_to_hf ./qwen1_5_1_8b_chat_qlora_alpaca_e3.<span class="property">py</span> ./iter_9560.<span class="property">pth</span> ./hf</span><br><span class="line">[<span class="number">2025</span>-<span class="number">06</span>-<span class="number">28</span> <span class="number">13</span>:<span class="number">28</span>:<span class="number">45</span>,<span class="number">006</span>] [<span class="variable constant_">INFO</span>] [real_accelerator.<span class="property">py</span>:<span class="number">222</span>:get_accelerator] <span class="title class_">Setting</span> ds_accelerator to <span class="title function_">cuda</span> (auto detect)</span><br><span class="line">[<span class="number">2025</span>-<span class="number">06</span>-<span class="number">28</span> <span class="number">13</span>:<span class="number">28</span>:<span class="number">49</span>,<span class="number">879</span>] [<span class="variable constant_">INFO</span>] [real_accelerator.<span class="property">py</span>:<span class="number">222</span>:get_accelerator] <span class="title class_">Setting</span> ds_accelerator to <span class="title function_">cuda</span> (auto detect)</span><br><span class="line"><span class="title class_">Traceback</span> (most recent call last):</span><br><span class="line">  <span class="title class_">File</span> <span class="string">&quot;/root/autodl-tmp/xtuner/xtuner/tools/model_converters/pth_to_hf.py&quot;</span>, line <span class="number">151</span>, <span class="keyword">in</span> &lt;<span class="variable language_">module</span>&gt;</span><br><span class="line">    <span class="title function_">main</span>()</span><br><span class="line">  <span class="title class_">File</span> <span class="string">&quot;/root/autodl-tmp/xtuner/xtuner/tools/model_converters/pth_to_hf.py&quot;</span>, line <span class="number">123</span>, <span class="keyword">in</span> main</span><br><span class="line">    state_dict = <span class="title function_">guess_load_checkpoint</span>(args.<span class="property">pth_model</span>)</span><br><span class="line">  <span class="title class_">File</span> <span class="string">&quot;/root/autodl-tmp/xtuner/xtuner/model/utils.py&quot;</span>, line <span class="number">314</span>, <span class="keyword">in</span> guess_load_checkpoint</span><br><span class="line">    state_dict = torch.<span class="title function_">load</span>(pth_model, map_location=<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">  <span class="title class_">File</span> <span class="string">&quot;/root/miniconda3/envs/xtuner-env/lib/python3.10/site-packages/torch/serialization.py&quot;</span>, line <span class="number">1524</span>, <span class="keyword">in</span> load</span><br><span class="line">    raise pickle.<span class="title class_">UnpicklingError</span>(<span class="title function_">_get_wo_message</span>(<span class="title function_">str</span>(e))) <span class="keyword">from</span> <span class="title class_">None</span></span><br><span class="line">_pickle.<span class="property">UnpicklingError</span>: <span class="title class_">Weights</span> only load failed. <span class="title class_">This</span> file can still be loaded, to <span class="keyword">do</span> so you have two options, <span class="keyword">do</span> those steps only <span class="keyword">if</span> you trust the source <span class="keyword">of</span> the checkpoint. </span><br><span class="line">        (<span class="number">1</span>) <span class="title class_">In</span> <span class="title class_">PyTorch</span> <span class="number">2.6</span>, we changed the <span class="keyword">default</span> value <span class="keyword">of</span> the weights_only argument <span class="keyword">in</span> torch.<span class="property">load</span> <span class="keyword">from</span> <span class="title class_">False</span> to <span class="title class_">True</span>. <span class="title class_">Re</span>-running torch.<span class="property">load</span> <span class="keyword">with</span> weights_only set to <span class="title class_">False</span> will likely succeed, but it can result <span class="keyword">in</span> arbitrary code execution. <span class="title class_">Do</span> it only <span class="keyword">if</span> you got the file <span class="keyword">from</span> a trusted source.</span><br><span class="line">        (<span class="number">2</span>) <span class="title class_">Alternatively</span>, to load <span class="keyword">with</span> weights_only=<span class="title class_">True</span> please check the recommended steps <span class="keyword">in</span> the following error message.</span><br><span class="line">        <span class="title class_">WeightsUnpickler</span> <span class="attr">error</span>: <span class="title class_">Unsupported</span> <span class="attr">global</span>: <span class="variable constant_">GLOBAL</span> mmengine.<span class="property">logging</span>.<span class="property">history_buffer</span>.<span class="property">HistoryBuffer</span> was not an allowed <span class="variable language_">global</span> by <span class="keyword">default</span>. <span class="title class_">Please</span> use torch.<span class="property">serialization</span>.<span class="title function_">add_safe_globals</span>([mmengine.<span class="property">logging</span>.<span class="property">history_buffer</span>.<span class="property">HistoryBuffer</span>]) or the torch.<span class="property">serialization</span>.<span class="title function_">safe_globals</span>([mmengine.<span class="property">logging</span>.<span class="property">history_buffer</span>.<span class="property">HistoryBuffer</span>]) context manager to allowlist <span class="variable language_">this</span> <span class="variable language_">global</span> <span class="keyword">if</span> you trust <span class="variable language_">this</span> <span class="keyword">class</span>/<span class="keyword">function</span>.</span><br><span class="line"><span class="title class_">Check</span> the documentation <span class="keyword">of</span> torch.<span class="property">load</span> to learn more about types accepted by <span class="keyword">default</span> <span class="keyword">with</span> weights_only <span class="attr">https</span>:<span class="comment">//pytorch.org/docs/stable/generated/torch.load.html.</span></span><br></pre></td></tr></table></figure><p><strong>原因：</strong></p><p>这个错误是由于 PyTorch 2.6 改变了 <code>torch.load</code> 的默认行为导致的。新版本默认启用了 <code>weights_only=True</code> 参数以提高安全性，但这导致某些包含非标准对象的检查点文件无法加载。</p><p><strong>解决：</strong></p><p>修改 xtuner 源码（推荐）找到错误提示中的文件 <code>/root/autodl-tmp/xtuner/xtuner/model/utils.py</code>，在第314行附近修改 <code>torch.load</code> 调用：</p><figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 原来的代码</span><br><span class="line">state_dict = torch.<span class="title function_">load</span>(pth_model, map_location=<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"># 修改为</span><br><span class="line">state_dict = torch.<span class="title function_">load</span>(pth_model, map_location=<span class="string">&quot;cpu&quot;</span>, weights_only=<span class="title class_">False</span>)</span><br></pre></td></tr></table></figure><p><strong>再次执行：</strong></p><p><img src="/images/%E6%88%AA%E5%B1%8F2025-06-28_13.43.38.png" alt="截屏2025-06-28 13.43.38.png"></p><p><strong>模型合并：</strong></p><p><strong><code>xtuner convert merge /root/autodl-tmp/models/Qwen/Qwen1___5-1___8B-Chat ./hf ./merged --max-shard-size 2GB</code></strong></p><p><img src="/images/%E6%88%AA%E5%B1%8F2025-06-28_13.47.24.png" alt="截屏2025-06-28 13.47.24.png"></p><p><strong>模型聊天：</strong></p><p><code>xtuner chat ./merged --prompt-template qwen_chat</code></p><blockquote><p>**<code>--prompt-template 参数选择:** choose from &#39;default&#39;, &#39;zephyr&#39;, &#39;internlm_chat&#39;, &#39;internlm2_chat&#39;, &#39;moss_sft&#39;, &#39;llama2_chat&#39;, &#39;code_llama_chat&#39;, &#39;chatglm2&#39;, &#39;chatglm3&#39;, &#39;qwen_chat&#39;, &#39;baichuan_chat&#39;, &#39;baichuan2_chat&#39;, &#39;wizardlm&#39;, &#39;wizardcoder&#39;, &#39;vicuna&#39;, &#39;deepseek_coder&#39;, &#39;deepseekcoder&#39;, &#39;deepseek_moe&#39;, &#39;deepseek_v2&#39;, &#39;mistral&#39;, &#39;mixtral&#39;, &#39;minicpm&#39;, &#39;minicpm3&#39;, &#39;gemma&#39;, &#39;cohere_chat&#39;, &#39;llama3_chat&#39;, &#39;phi3_chat’</code></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> NOTE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Xtuner </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2025搭建 github + Hexo 个人博客</title>
      <link href="/2025/07/02/2025%E6%90%AD%E5%BB%BA-github-Hexo-%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
      <url>/2025/07/02/2025%E6%90%AD%E5%BB%BA-github-Hexo-%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Hexo安装"><a href="#1-Hexo安装" class="headerlink" title="1.Hexo安装"></a>1.Hexo安装</h1><h4 id="以macOS为例"><a href="#以macOS为例" class="headerlink" title="以macOS为例"></a>以macOS为例</h4><ul><li><p>安装Hexo：<code>npm install -g hexo-cli</code></p></li><li><p>选好要安装的目录：<code>your/custom/path</code>,然后进入该目录<code>cd your/custom/path</code></p></li><li><p>然后初始化Hexo：<code>hexo init blog</code></p></li><li><p>进入blog文件夹: <code>cd blog</code></p></li><li><p>安装依赖：<code>npm install</code></p></li><li><p>启动服务：<code>hexo server</code></p></li><li><p>访问：<code>localhost:4000</code></p><p><img src="/images/image-20250702213318274.png" alt="image-20250702213318274"></p></li></ul><h1 id="2-主题配置"><a href="#2-主题配置" class="headerlink" title="2.主题配置"></a>2.主题配置</h1><blockquote><p>主题：<a href="https://hexo.io/themes/">https://hexo.io/themes/</a></p></blockquote><ul><li><p>在<code>blog</code>目录下：</p><ul><li><p>首先：<code>git init</code></p></li><li><p>然后：<code>git submodule add https://github.com/Your/Hexo_Theme.git themes/Hexo_Theme_Name</code></p></li><li><p>修改<code>blog</code>目录下的<code>_config.yml</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">theme:</span> <span class="string">Hexo_Theme_Name</span></span><br></pre></td></tr></table></figure></li></ul></li></ul><h1 id="3-部署到Github"><a href="#3-部署到Github" class="headerlink" title="3.部署到Github"></a>3.部署到Github</h1><ul><li>修改<code>blog</code>目录下的<code>_config.yml</code></li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">deploy:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">  <span class="comment"># repo 建议使用ssh的方式 自行搜索或者AI 仓库提前创建，仓库名：your_github_username.github.io</span></span><br><span class="line">  <span class="attr">repo:</span> <span class="string">git@github.com:your_github_username/your_github_username.github.io.git</span> </span><br><span class="line">  <span class="attr">branch:</span> <span class="string">master</span></span><br></pre></td></tr></table></figure><ul><li>安装<code>deploy-git</code>: <code>npm install hexo-deployer-git --save</code></li><li>然后：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean #清除之前生成的东西</span><br><span class="line">hexo generate  #生成静态文章，缩写hexo g</span><br><span class="line">hexo deploy  #部署文章，缩写hexo d</span><br></pre></td></tr></table></figure><p>部署完成，访问<code>your_github_username.github.io</code>即可</p><ul><li>使用自己的域名：<code>your_github_username.github.io -&gt; Settings -&gt; Pages -&gt; Custom domain</code></li></ul><blockquote><p>使用自己的域名首先解析一下域名</p></blockquote><p><img src="/images/image-20250702220239010.png" alt="image-20250702220239010"></p><p>使用<code>hexo d</code>部署完后域名会失效，需在<code>source</code>文件夹下新建一个<code>CNAME</code>文件，内容为你的域名。</p>]]></content>
      
      
      <categories>
          
          <category> TUTORIAL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
